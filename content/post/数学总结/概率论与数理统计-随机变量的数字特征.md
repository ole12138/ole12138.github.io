---
title: "概率论与数理统计-随机变量及其分布"
date: 2020-11-27T09:24:48+08:00
draft: false
categories: ["数学"]
tags: ["概率论", "随机变量", "随机变量的数字特征", "数学期望", "均值", "方差", "协方差", "相关系数", "矩", "协方差矩阵", "n维正态分布", "二次型", "矩阵合同"]
markup: pandoc
---

# 概率论与数理统计-随机变量的数字特征

随机变量及其概率分布章节，我们讨论了**随机变量的分布函数**，它完整反映了随机变量（在一定范围内）的概率特性。

然而很多时候，我们不需要或者很难知道随机变量的完整特性，只需要知道它的某些重要指标。
（比如在调查一群人的身高或者某地居民的收入时，我们经常关心这些量的均值或者个体之间的差异。在碰到多个随机变量时，还需要一些指标来反映这些随机变量之间的关系。）

与随机变量密切相关的指标统称为**数字特征**。重要的数字特征有数学期望、方差、协方差、相关系数以及矩等.

## 数学期望

### 离散型随机变量的数学期望

#### 离散型随机变量的数学期望的定义

设离散型随机变量X的概率分布律为$P\left(X=x_{k}\right)=p\left(x_{k}\right), \quad k=1,2, \cdots$
如果无穷级数$\sum_{k=1}^{\infty} x_{k} p\left(x_{k}\right)$绝对收敛，
则称$\sum_{k=1}^{\infty} x_{k} p\left(x_{k}\right)$为**（离散型）随机变量X的数学期望**，简称数学期望或均值，
记作 $E(X)$，即$E(X)=\sum_{k=1}^{\infty} x_{k} p\left(x_{k}\right)$

#### 常见离散型随机变量的数学期望

##### 0-1分布的期望$E(X)=p$

> 设X服从参数为p的两点分布（0-1分布），即$P(X=0)=1-p, \quad P(X=1)=p, \quad 0<p<1$
> 则$E(X)=1 \times p+0 \times(1-p)=p$

##### 二项分布$X \sim B(n, p)$的期望$E(X)=n p$

> 证明（方法一）：
>
> 因为$X \sim B(n, p)$，所以$P(X=k)=C_{n}^{k} p^{k}(1-p)^{n-k}, \quad k=0,1,2, \cdots, n$
> 则：
> $\begin{aligned} E(X) &=\sum_{k=0}^{n} k \mathrm{C}_{n}^{k} p^{k}(1-p)^{n-k}=\sum_{k=0}^{n} \frac{k n !}{k !(n-k) !} p^{k}(1-p)^{n-k} \\ &=n p \sum_{k=1}^{n} \frac{(n-1) !}{(k-1) ![(n-1)-(k-1)] !} p^{k-1}(1-p)^{(n-1)-(k-1)} \\ &=n p \sum_{k=1}^{n} C_{n-1}^{k-1} p^{k-1}(1-p)^{(n-1)-(k-1)} \end{aligned}$
> 令$m=k-1$,
> 则：
> $\begin{aligned} \sum_{k=1}^{n} \mathrm{C}_{n-1}^{k-1} p^{k-1}(1-p)^{(n-1)-(k-1)} &=\sum_{m=0}^{n-1} \mathrm{C}_{n-1}^{m} p^{m}(1-p)^{(n-1)-m} \\ &=[p+(1-p)]^{n-1} \\ &=1 \end{aligned}$
> 从而$E(X)=n p$

> 证明（方法二）：
> 若$X \sim B(n, p)$，则X可以表示成n个独立的0-1型随机变量的和，
> 即$X=X_{1}+X_{2}+\cdots+X_{n}$，其中$X_{i}=\left\{\begin{array}{ll}1, & \text { 第i次试验中A发生, } \\ 0, & \text { 否则. }\end{array}\right.$，$i=1,2, \cdots, n$
> 根据后面期望的性质：[$E\left(X_{1}+X_{2}+\cdots+X_{n}\right)=E\left(X_{1}\right)+E\left(X_{2}\right)+\cdots+E\left(X_{n}\right)$](#$E\left(X_{1}+X_{2}+\cdots+X_{n}\right)=E\left(X_{1}\right)+E\left(X_{2}\right)+\cdots+E\left(X_{n}\right)$)
> 有$E(X)=E\left(\sum_{i=1}^{n} X_{i}\right)=\sum_{i=1}^{n} E\left(X_{i}\right)=n p$

##### 几何分布 $X \sim G(p)$的期望$E(X)=\frac{1}{p}$

> 因为 $X \sim G(p),$ 所以$P(X=k)=(1-p)^{k-1} p, \quad k=1,2, \cdots$
> 则$E(X)=\sum_{k=1}^{\infty} k(1-p)^{k-1} p=p \sum_{k=1}^{\infty} k(1-p)^{k-1}$
>
> 根据高等数学幂级数的知识，有如下幂级数的和函数：
> $\sum_{k=0}^{\infty} x^{k}=\frac{1}{1-x}, \quad|x|<1$
> 根据幂级数的逐项可导性质，有：
> $\sum_{k=1}^{\infty} k x^{k-1}=\frac{1}{(1-x)^{2}}, \quad|x|<1$
> 取$x=1-p$,则：
> $\sum_{k=1}^{\infty} k(1-p)^{k-1}=\frac{1}{p^{2}}$
> 因此$E(X)=p \sum_{k=1}^{\infty} k(1-p)^{k-1}=\frac{1}{p}$

##### 泊松分布$X\sim P(\lambda)$的期望$E(X)=\lambda$

> 因为$X\sim P(\lambda)$，则$P(X=k)=\frac{\lambda^{k}}{k !} \mathrm{e}^{-\lambda}, \quad k=0,1,2, \cdots$
> 则$E(X)=\sum_{k=0}^{\infty} k \frac{\lambda^{k} \mathrm{e}^{-\lambda}}{k !}=\lambda \mathrm{e}^{-\lambda} \sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1) !}=\lambda \mathrm{e}^{-\lambda} \mathrm{e}^{\lambda}=\lambda$

### 连续型随机变量的数学期望

#### 连续型随机变量数学期望的定义

设连续型随机变量 $X$ 的密度函数为 $f(x) .$ 
若积分 $\int_{-\infty}^{+\infty} x f(x) \mathrm{d} x$ 绝对收敛,则称 $\int_{-\infty}^{+\infty} x f(x) \mathrm{d} x$ 为**连续型随机变量 $X$ 的数学期望**, 简称**期望**或**均值**, 
记作 $E(X),$ 即$E(X)=\int_{-\infty}^{+\infty} x f(x) \mathrm{d} x$

注意：某些分布的数学期望可能不存在。（密度函数不绝对收敛）

#### 常见连续型随机变量的数学期望

##### 均匀分布$X \sim U[a, b]$的期望$E(X)=\frac{a+b}{2}$

> 因为$X \sim U[a, b]$，所以X的密度函数为：
> $f(x)=\left\{\begin{array}{ll}\frac{1}{b-a}, & a \leqslant x \leqslant b \\ 0, & \text { 其他. }\end{array}\right.$
> 于是：
> $E(X)=\int_{-\infty}^{+\infty} x f(x) \mathrm{d} x=\int_{a}^{b} \frac{x}{b-a} \mathrm{~d} x=\frac{a+b}{2}$ 

##### 指数分布$X \sim e(\lambda)$的期望$E(X)=\frac{1}{\lambda}$

> 因为$X \sim e(\lambda)$，所以X的密度函数为$f(x)=\left\{\begin{array}{ll}\lambda \mathrm{e}^{-\lambda x}, & x \geqslant 0 \\ 0, & x<0\end{array}\right.$
> 于是$E(X)=\int_{-\infty}^{+\infty} x f(x) \mathrm{d} x=\int_{0}^{+\infty} x \lambda \mathrm{e}^{-\lambda x} \mathrm{~d} x=\frac{1}{\lambda}$

##### 正态分布$X \sim N\left(\mu, \sigma^{2}\right)$的期望$E(X)=\mu$

> 因为$X \sim N\left(\mu, \sigma^{2}\right)$，所以X的密度函数为$f(x)=\frac{1}{\sqrt{2 \pi} \sigma} \mathrm{e}^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}, \quad-\infty<x<+\infty$
> 于是$E(X)=\int_{-\infty}^{+\infty} x \frac{1}{\sqrt{2 \pi} \sigma} \mathrm{e}^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}} \mathrm{~d} x$
> 令 $t=\frac{x-\mu}{\sigma},$ 则：
> $\begin{aligned} E(X) &=\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty}(\sigma t+\mu) \mathrm{e}^{-\frac{t^{2}}{2}} \mathrm{~d} t \\ &=\frac{\mu}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} \mathrm{e}^{-\frac{t^{2}}{2}} \mathrm{~d} t \\ &=\mu \end{aligned}$

### 随机变量函数的数学期望

#### 随机变量X的函数$g(X)$的期望

很多时候需要计算随机变量X的某个函数$g(X)$的期望。

方法一：自然会想到先求$g(X)$的分布，再按期望的定义来求$E[g(X)]$.

方法二：但是很多时候，$g(X)$的分布并不容易求出，下面的定理给出另一种方法。

> 定理：设X为一个随机变量，$g(x)$为一元函数，
> 1）如果X为离散型随机变量，分布律为$P\left(X=x_{k}\right)=p\left(x_{k}\right), \quad k=1,2, \cdots$，
> 则$E[g(X)]=\sum_{k=1}^{\infty} g\left(x_{k}\right) p\left(x_{k}\right)$
> 2）如果X为连续性随机变量，密度函数为$f(x)$，
> 则$E[g(X)]=\int_{-\infty}^{+\infty} g(x) f(x) \mathrm{d} x$

这个定理告诉我们，由X的分布即可求出其函数$g(X)$的期望，而无需求出$g(X)$的分布。

#### 二维随机变量$(X,Y)$的函数$g(X,Y)$的期望

类似一维的情况，有两种方法。

方法一：自然会想到先求$g(X,Y)$的分布，再按期望的定义来求$E[g(X,Y)]$.

方法二：但是很多时候，$g(X,Y)$的分布并不容易求出，根据下面的定理，由$(X,Y)$的分布即可直接求出其函数$Z=g(X,Y)$的期望

> 定理：设$(X,Y)$为二维随机变量，$g(x,y)$为二元函数。
> 1）如果$(X,Y)$为二维离散型随机变量，联合分布律为$P\left(X=x_{i}, Y=y_{j}\right)=p_{i j}, \quad i=1,2, \cdots, \quad j=1,2, \cdots$，
> 则$E[g(X, Y)]=\sum_{i=1}^{\infty} \sum_{j=1}^{\infty} g\left(x_{i}, y_{j}\right) p_{i j}$
> 2）如果$(X,Y)$为二维离散型随机变量，联合密度函数为$f(x,y)$，
> 则$E[g(X, Y)]=\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} g(x, y) f(x, y) \mathrm{d} x \mathrm{~d} y$



特别的，我们取$g(X,Y) = X$和$g(X,Y)=Y$。即已知$(X,Y)$分布或者密度函数时可求$E(X)$和$E(Y)$.

可以按照方法二求解：

> 以二维连续型为例，设联合密度函数为$f(x,y)$，则：
> $E(X)=\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} x f(x, y) \mathrm{d} x \mathrm{~d} y$
> $E(Y)=\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} y f(x, y) \mathrm{d} x \mathrm{d} y$

当然也可以按照方法一求解：

> 先求X和Y各自的边缘密度$f_X(x)$和$f_Y(y)$，
> 再利用$E(X)=\int_{-\infty}^{+\infty} x f_{X}(x) \mathrm{d} x$和$E(Y)=\int_{-\infty}^{+\infty} y f_{Y}(y) \mathrm{d} y$求解

#### 高维随机变量$(X_1,X_2, \cdots, X_n)$的函数$g(X_1,X_2, \cdots, X_n)$的期望

类似二维的情况，有相同的两种方法。并且有类似的定理存在。

### 期望的性质

#### $E(C)=C$

设C为常数，则$E(C)=C$

#### $E(kX)= k E(X)$

设k为常数，X为随机变量，则$E(kX)= k E(X)$

#### $E(X+Y)=E(X)+E(Y)$

对于随机变量X和Y，有$E(X+Y)=E(X)+E(Y)$

#### $E\left(X_{1}+X_{2}+\cdots+X_{n}\right)=E\left(X_{1}\right)+E\left(X_{2}\right)+\cdots+E\left(X_{n}\right)$

这是上一条的推论：对于n个随机变量$X_{1}+X_{2}+\cdots+X_{n}$，有$E\left(X_{1}+X_{2}+\cdots+X_{n}\right)=E\left(X_{1}\right)+E\left(X_{2}\right)+\cdots+E\left(X_{n}\right)$

#### 随机变量X与Y独立$\Rightarrow$ $E(X Y)=E(X) E(Y)$

设随机变量X和Y独立，则$E(X Y)=E(X) E(Y)$

> 证明：
> 二维连续型$(X,Y)$情况（离散型类似可证）
> 设$(X,Y)$的联合密度函数为$f(x,y)$,
> 边缘密度函数微分别$f_X(x)$和$f_Y(y)$,
> 由于X和Y独立，则$f(x,y)=f_X(x)f_Y(y)$，
> 则：
> $\begin{aligned} E(X Y) &=\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} x y f(x, y) \mathrm{d} x \mathrm{~d} y \\ &=\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} x y f_{X}(x) f_{Y}(y) \mathrm{d} x \mathrm{~d} y \\ &=\left(\int_{-\infty}^{+\infty} x f_{X}(x) \mathrm{d} x\right)\left(\int_{-\infty}^{+\infty} y f_{Y}(y) \mathrm{d} y\right) \\ &=E(X) E(Y) \end{aligned}$

#### $X_1,X_2, \cdots, X_n$相互独立$\Rightarrow$ $E\left(X_{1} X_{2} \cdots X_{n}\right)=E\left(X_{1}\right) E\left(X_{2}\right) \cdots E\left(X_{n}\right)$

这是上一条的推论：设n个随机变量$X_1,X_2, \cdots, X_n$相互独立，则$E\left(X_{1} X_{2} \cdots X_{n}\right)=E\left(X_{1}\right) E\left(X_{2}\right) \cdots E\left(X_{n}\right)$

## 方差

随机变量的期望反应的是随机变量的均值。除了均值之外，人们还常关心随机变量取值的波动性或者稳定性如何。
衡量波动性的一个简单办法是看随机变量每个可能取值与期望值的差距$|X-E(X)|$,很显然这是一个随机变量。
因此想到用$|X-E(X)|$的均值$E\left[|X-E(X)|\right]$来反映波动性，
但绝对值在数学上比较难处理，因此实际中使用$E\left[X-E(X)\right]^2$来**反映随机变量波动性的大小**，也即反映随机变量稳定性如何，由此引出方差的定义。

### 方差的定义与常用公式

#### 方差的定义$D(X)=E\left[X-E(X)\right]^2$

设X为一随机变量，若$E\left[X-E(X)\right]^2$存在，则称$E\left[X-E(X)\right]^2$为随机变量X的方差，记为$D(X)$，
即$D(X)=E\left[X-E(X)\right]^2$

注意：随机变量X的方差$D(X)$结果是个数值。

#### 方差的常用式$D(X)=E\left(X^{2}\right)-[E(X)]^{2}$

> 证明
> 利用期望的性质，则
> $\begin{aligned} D(X) &=E[X-E(X)]^{2}=E\left\{X^{2}-2 X E(X)+[E(X)]^{2}\right\} \\ &=E\left(X^{2}\right)-2[E(X)]^{2}+[E(X)]^{2} \\ &=E\left(X^{2}\right)-[E(X)]^{2} \end{aligned}$

### 离散型随机变量的方差

设离散型随机变量X的概率分布律为$P\left(X=x_{k}\right)=p\left(x_{k}\right), \quad k=1,2, \cdots$，
则随机变量X的方差
$D(X)=E\left[X-E(X)\right]^2 \\=\sum_{i=1}^{\infty}\left[x_{i}-E(X)\right]^{2} p\left(x_{i}\right)$

也常利用$D(X)=E\left(X^{2}\right)-[E(X)]^{2}$来计算某些离散型随机变量的方差。

#### 0-1分布的方差$D(X)=p(1-p)$

> 证明
> 设X服从参数为p的两点分布（0-1分布），[显然$E(X)=p$](#0-1分布的期望$E(X)=p$),
> 又$E\left(X^{2}\right)=1^{2} \times p+0^{2} \times(1-p)=p$，
> 则$D(X)=E\left(X^{2}\right)-[E(X)]^{2}=p(1-p)$

#### 二项分布$X \sim B(n, p)$的方差$D(X)=n p(1-p)$

> 证明（要用到后面方差的性质）：
>
> 我们知道$X=X_{1}+X_{2}+\cdots+X_{n}$， （参见[二项分布$X \sim B(n, p)$期望$E(X)=n p$的证明方法二](#二项分布$X \sim B(n, p)$的期望$E(X)=n p$)）
> 而[0-1分布的方差$D(X_i)=p(1-p)$](#0-1分布的方差$D(X)=p(1-p)$),
> 又$X_1,X_2, \cdots, X_n$相互独立，
> 于是根据[$X_1,X_2, \cdots, X_n$相互独立$\Rightarrow$ $D\left(X_{1}+X_{2}+\cdots+X_{n}\right)=D\left(X_{1}\right)+D\left(X_{2}\right)+\cdots+D\left(X_{n}\right)$](#随机变量$X_1,X_2, \cdots, X_n$相互独立$\Rightarrow$ $D\left(X_{1}+X_{2}+\cdots+X_{n}\right)=D\left(X_{1}\right)+D\left(X_{2}\right)+\cdots+D\left(X_{n}\right)$)
> 得$D(X)=D\left(\sum_{i=1}^{n} X_{i}\right)=\sum_{i=1}^{n} D\left(X_{i}\right)=n p(1-p)$

#### 泊松分布$X\sim P(\lambda)$的方差$D(X)=\lambda$

> 我们知道[泊松分布$X\sim P(\lambda)$的期望$E(X)=\lambda$](#泊松分布$X\sim P(\lambda)$的期望$E(X)=\lambda$)
> 而：
> $\begin{aligned} E\left(X^{2}\right) &=\sum_{k=0}^{\infty} k^{2} \frac{\lambda^{k}}{k !} \mathrm{e}^{-\lambda}=\sum_{k=1}^{\infty} k \frac{\lambda^{k}}{(k-1) !} \mathrm{e}^{-\lambda} \\ &=\sum_{k=1}^{\infty}[(k-1)+1] \frac{\lambda^{k}}{(k-1) !} \mathrm{e}^{-\lambda} \\ &=\sum_{k=2}^{\infty} \frac{\lambda^{k}}{(k-2) !} \mathrm{e}^{-\lambda}+\sum_{k=1}^{\infty} \frac{\lambda^{k}}{(k-1) !} \mathrm{e}^{-\lambda} \end{aligned}$
> 又有$\sum_{k=1}^{\infty} \frac{\lambda^{k}}{(k-1) !} \mathrm{e}^{-\lambda}=\lambda \cdot\sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1) !} \mathrm{e}^{-\lambda}=\lambda$ 
> 以及$\sum_{k=2}^{\infty} \frac{\lambda^{k}}{(k-2) !} \mathrm{e}^{-\lambda}=\lambda^{2} \mathrm{e}^{-\lambda} \sum_{k=2}^{\infty} \frac{\lambda^{k-2}}{(k-2) !}=\lambda^{2} \mathrm{e}^{-\lambda} \mathrm{e}^{\lambda}=\lambda^{2}$
> 所以$E\left(X^{2}\right)=\lambda^{2}+\lambda$
> 则$D(X)=E\left(X^{2}\right)-[E(X)]^{2}=\lambda$

### 连续型随机变量的方差

设连续型随机变量 $X$ 的密度函数为 $f(x) .$ 
则随机变量X的方差
$D(X)=\int_{-\infty}^{+\infty}[x-E(X)]^{2} f(x) \mathrm{d} x$

#### 均匀分布$X \sim U[a, b]$的方差$D(X)=\frac{(b-a)^{2}}{12}$

> $X \sim U[a, b]$，
> 前面已求出$E(X)=\frac{a+b}{2}$，
> 而$E\left(X^{2}\right)=\int_{a}^{b} \frac{x^{2}}{b-a} \mathrm{~d} x=\frac{a^{2}+a b+b^{2}}{3}$
> 从而$D(X)=E\left(X^{2}\right)-[E(X)]^{2}=\frac{(b-a)^{2}}{12}$

#### 指数分布$X \sim e(\lambda)$的方差$D(X)=\frac{1}{\lambda^{2}}$

> 因为$X \sim e(\lambda)$，
> 前面已经求得$E(X)=\frac{1}{\lambda}$，
> 而随机变量函数$X^2$的期望$E\left(X^{2}\right)=\int_{0}^{+\infty} x^{2} \lambda \mathrm{e}^{-\lambda x} \mathrm{~d} x=\frac{2}{\lambda^{2}}$
> 从而$D(X)=E\left(X^{2}\right)-[E(X)]^{2}=\frac{1}{\lambda^{2}}$

#### 正态分布$X \sim N\left(\mu, \sigma^{2}\right)$的方差$D(X)=\sigma^{2}$

> $X \sim N\left(\mu, \sigma^{2}\right)$，
> 前面已求出$E(X)=\mu$，
> 而$E\left(X^{2}\right)=\int_{-\infty}^{+\infty} x^{2} \frac{1}{\sqrt{2 \pi} \sigma} \mathrm{e}^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}} \mathrm{~d} x$，
> 令$t=\frac{x-\mu}{\sigma}$，利用分部积分法容易求得$E\left(X^{2}\right)=\sigma^{2}+\mu^{2}$，
>
> 从而$D(X)=E\left(X^{2}\right)-[E(X)]^{2}=\sigma^{2}$

### 方差的性质

方差就具有如下一些重要性质（设所涉及的随机变量的方差均存在）

#### $D(C)=0$

设 $C$ 为一常数, $X$ 为一随机变量, 则$D(C)=0$

#### $D(k X)=k^{2} D(X)$

设 $k$ 为一常数, $X$ 为一随机变量, 则$D(k X)=k^{2} D(X)$

> 证明
> $D(k X)=E\left(k^{2} X^{2}\right)-[E(k X)]^{2}\\=k^{2} E\left(X^{2}\right)-k^{2}[E(X)]^{2}\\=k^{2} D(X)$

#### 随机变量X与Y相互独立$\Rightarrow$ $D(X+Y)=D(X)+D(Y)$

对随机变量 $X$ 和 $Y$ 相互独立, 则$D(X+Y)=D(X)+D(Y)$

> 证明
> $\begin{aligned} D(X+Y) &=E(X+Y)^{2}-[E(X+Y)]^{2} \\ &=E\left(X^{2}+2 X Y+Y^{2}\right)-\left\{[E(X)]^{2}+2 E(X) E(Y)+[E(Y)]^{2}\right\} \\ &=E\left(X^{2}\right)-[E(X)]^{2}+E\left(Y^{2}\right)-[E(Y)]^{2}+2[E(X Y)-E(X) E(Y)] \\ &=D(X)+D(Y)+2[E(X Y)-E(X) E(Y)] \end{aligned}$
> X与Y相互独立，由[随机变量X与Y独立$\Rightarrow$ $E(X Y)=E(X) E(Y)$](#随机变量X与Y独立$\Rightarrow$ $E(X Y)=E(X) E(Y)$)可知 $E(X Y)-E(X) E(Y)=0$，
> 从而$D(X+Y)=D(X)+D(Y)$

#### 随机变量$X_1,X_2, \cdots, X_n$相互独立$\Rightarrow$ $D\left(X_{1}+X_{2}+\cdots+X_{n}\right)=D\left(X_{1}\right)+D\left(X_{2}\right)+\cdots+D\left(X_{n}\right)$

这是上一条的推论。

## 协方差与相关系数

## 矩

## 协方差矩阵

## n维正态分布