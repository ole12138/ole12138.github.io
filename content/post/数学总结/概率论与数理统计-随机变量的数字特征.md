---
title: "概率论与数理统计-随机变量及其分布"
date: 2020-11-27T09:24:48+08:00
draft: false
categories: ["数学"]
tags: ["概率论", "随机变量", "随机变量的数字特征", "数学期望", "均值", "方差", "协方差", "相关系数", "矩", "协方差矩阵", "n维正态分布", "二次型", "矩阵合同"]
markup: pandoc
---

# 概率论与数理统计-随机变量的数字特征

随机变量及其概率分布章节，我们讨论了**随机变量的分布函数**，它完整反映了随机变量（在一定范围内）的概率特性。

然而很多时候，我们不需要或者很难知道随机变量的完整特性，只需要知道它的某些重要指标。
（比如在调查一群人的身高或者某地居民的收入时，我们经常关心这些量的均值或者个体之间的差异。在碰到多个随机变量时，还需要一些指标来反映这些随机变量之间的关系。）

与随机变量密切相关的指标统称为**数字特征**。重要的数字特征有数学期望、方差、协方差、相关系数以及矩等.

## 数学期望

### 离散型随机变量的数学期望

#### 离散型随机变量的数学期望的定义

设离散型随机变量X的概率分布律为$P\left(X=x_{k}\right)=p\left(x_{k}\right), \quad k=1,2, \cdots$
如果无穷级数$\sum_{k=1}^{\infty} x_{k} p\left(x_{k}\right)$绝对收敛，
则称$\sum_{k=1}^{\infty} x_{k} p\left(x_{k}\right)$为**（离散型）随机变量X的数学期望**，简称数学期望或均值，
记作 $E(X)$，即$E(X)=\sum_{k=1}^{\infty} x_{k} p\left(x_{k}\right)$

#### 常见离散型随机变量的数学期望

##### 0-1分布的期望$E(X)=p$

> 设X服从参数为p的两点分布（0-1分布），即$P(X=0)=1-p, \quad P(X=1)=p, \quad 0<p<1$
> 则$E(X)=1 \times p+0 \times(1-p)=p$

##### 二项分布$X \sim B(n, p)$的期望$E(X)=n p$

> 证明（方法一）：
>
> 因为$X \sim B(n, p)$，所以$P(X=k)=C_{n}^{k} p^{k}(1-p)^{n-k}, \quad k=0,1,2, \cdots, n$
> 则：
> $\begin{aligned} E(X) &=\sum_{k=0}^{n} k \mathrm{C}_{n}^{k} p^{k}(1-p)^{n-k}=\sum_{k=0}^{n} \frac{k n !}{k !(n-k) !} p^{k}(1-p)^{n-k} \\ &=n p \sum_{k=1}^{n} \frac{(n-1) !}{(k-1) ![(n-1)-(k-1)] !} p^{k-1}(1-p)^{(n-1)-(k-1)} \\ &=n p \sum_{k=1}^{n} C_{n-1}^{k-1} p^{k-1}(1-p)^{(n-1)-(k-1)} \end{aligned}$
> 令$m=k-1$,
> 则：
> $\begin{aligned} \sum_{k=1}^{n} \mathrm{C}_{n-1}^{k-1} p^{k-1}(1-p)^{(n-1)-(k-1)} &=\sum_{m=0}^{n-1} \mathrm{C}_{n-1}^{m} p^{m}(1-p)^{(n-1)-m} \\ &=[p+(1-p)]^{n-1} \\ &=1 \end{aligned}$
> 从而$E(X)=n p$

证明（方法二）：
要用到后面期望的性质：[$X_1,X_2, \cdots, X_n$相互独立$\Rightarrow$ $E\left(X_{1} X_{2} \cdots X_{n}\right)=E\left(X_{1}\right) E\left(X_{2}\right) \cdots E\left(X_{n}\right)$](#$X_1,X_2, \cdots, X_n$相互独立$\Rightarrow$ $E\left(X_{1} X_{2} \cdots X_{n}\right)=E\left(X_{1}\right) E\left(X_{2}\right) \cdots E\left(X_{n}\right)$)

##### 几何分布 $X \sim G(p)$的期望$E(X)=\frac{1}{p}$

> 因为 $X \sim G(p),$ 所以$P(X=k)=(1-p)^{k-1} p, \quad k=1,2, \cdots$
> 则$E(X)=\sum_{k=1}^{\infty} k(1-p)^{k-1} p=p \sum_{k=1}^{\infty} k(1-p)^{k-1}$
>
> 根据高等数学幂级数的知识，有如下幂级数的和函数：
> $\sum_{k=0}^{\infty} x^{k}=\frac{1}{1-x}, \quad|x|<1$
> 根据幂级数的逐项可导性质，有：
> $\sum_{k=1}^{\infty} k x^{k-1}=\frac{1}{(1-x)^{2}}, \quad|x|<1$
> 取$x=1-p$,则：
> $\sum_{k=1}^{\infty} k(1-p)^{k-1}=\frac{1}{p^{2}}$
> 因此$E(X)=p \sum_{k=1}^{\infty} k(1-p)^{k-1}=\frac{1}{p}$

##### 泊松分布$X\sim P(\lambda)$的期望$E(X)=\lambda$

> 因为$X\sim P(\lambda)$，则$P(X=k)=\frac{\lambda^{k}}{k !} \mathrm{e}^{-\lambda}, \quad k=0,1,2, \cdots$
> 则$E(X)=\sum_{k=0}^{\infty} k \frac{\lambda^{k} \mathrm{e}^{-\lambda}}{k !}=\lambda \mathrm{e}^{-\lambda} \sum_{k=1}^{\infty} \frac{\lambda^{k-1}}{(k-1) !}=\lambda \mathrm{e}^{-\lambda} \mathrm{e}^{\lambda}=\lambda$

### 连续型随机变量的数学期望

#### 连续型随机变量数学期望的定义

设连续型随机变量 $X$ 的密度函数为 $f(x) .$ 
若积分 $\int_{-\infty}^{+\infty} x f(x) \mathrm{d} x$ 绝对收敛,则称 $\int_{-\infty}^{+\infty} x f(x) \mathrm{d} x$ 为**连续型随机变量 $X$ 的数学期望**, 简称**期望**或**均值**, 
记作 $E(X),$ 即$E(X)=\int_{-\infty}^{+\infty} x f(x) \mathrm{d} x$

注意：某些分布的数学期望可能不存在。（密度函数不绝对收敛）

#### 常见连续型随机变量的数学期望

##### 均匀分布$X \sim U[a, b]$的期望$E(X)=\frac{a+b}{2}$

> 因为$X \sim U[a, b]$，所以X的密度函数为：
> $f(x)=\left\{\begin{array}{ll}\frac{1}{b-a}, & a \leqslant x \leqslant b \\ 0, & \text { 其他. }\end{array}\right.$
> 于是：
> $E(X)=\int_{-\infty}^{+\infty} x f(x) \mathrm{d} x=\int_{a}^{b} \frac{x}{b-a} \mathrm{~d} x=\frac{a+b}{2}$ 

##### 指数分布$X \sim e(\lambda)$的期望$E(X)=\frac{1}{\lambda}$

> 因为$X \sim e(\lambda)$，所以X的密度函数为$f(x)=\left\{\begin{array}{ll}\lambda \mathrm{e}^{-\lambda x}, & x \geqslant 0 \\ 0, & x<0\end{array}\right.$
> 于是$E(X)=\int_{-\infty}^{+\infty} x f(x) \mathrm{d} x=\int_{0}^{+\infty} x \lambda \mathrm{e}^{-\lambda x} \mathrm{~d} x=\frac{1}{\lambda}$

##### 正态分布$X \sim N\left(\mu, \sigma^{2}\right)$的期望$E(X)=\mu$

> 因为$X \sim N\left(\mu, \sigma^{2}\right)$，所以X的密度函数为$f(x)=\frac{1}{\sqrt{2 \pi} \sigma} \mathrm{e}^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}, \quad-\infty<x<+\infty$
> 于是$E(X)=\int_{-\infty}^{+\infty} x \frac{1}{\sqrt{2 \pi} \sigma} \mathrm{e}^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}} \mathrm{~d} x$
> 令 $t=\frac{x-\mu}{\sigma},$ 则：
> $\begin{aligned} E(X) &=\frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty}(\sigma t+\mu) \mathrm{e}^{-\frac{t^{2}}{2}} \mathrm{~d} t \\ &=\frac{\mu}{\sqrt{2 \pi}} \int_{-\infty}^{+\infty} \mathrm{e}^{-\frac{t^{2}}{2}} \mathrm{~d} t \\ &=\mu \end{aligned}$

### 随机变量函数的数学期望

#### 随机变量X的函数$g(X)$的期望

很多时候需要计算随机变量X的某个函数$g(X)$的期望。

方法一：自然会想到先求$g(X)$的分布，再按期望的定义来求$E[g(X)]$.

方法二：但是很多时候，$g(X)$的分布并不容易求出，下面的定理给出另一种方法。

> 定理：设X为一个随机变量，$g(x)$为一元函数，
> 1）如果X为离散型随机变量，分布律为$P\left(X=x_{k}\right)=p\left(x_{k}\right), \quad k=1,2, \cdots$，
> 则$E[g(X)]=\sum_{k=1}^{\infty} g\left(x_{k}\right) p\left(x_{k}\right)$
> 2）如果X为连续性随机变量，密度函数为$f(x)$，
> 则$E[g(X)]=\int_{-\infty}^{+\infty} g(x) f(x) \mathrm{d} x$

这个定理告诉我们，由X的分布即可求出其函数$g(X)$的期望，而无需求出$g(X)$的分布。

#### 二维随机变量$(X,Y)$的函数$g(X,Y)$的期望

类似一维的情况，有两种方法。

方法一：自然会想到先求$g(X,Y)$的分布，再按期望的定义来求$E[g(X,Y)]$.

方法二：但是很多时候，$g(X,Y)$的分布并不容易求出，根据下面的定理，由$(X,Y)$的分布即可直接求出其函数$Z=g(X,Y)$的期望

> 定理：设$(X,Y)$为二维随机变量，$g(x,y)$为二元函数。
> 1）如果$(X,Y)$为二维离散型随机变量，联合分布律为$P\left(X=x_{i}, Y=y_{j}\right)=p_{i j}, \quad i=1,2, \cdots, \quad j=1,2, \cdots$，
> 则$E[g(X, Y)]=\sum_{i=1}^{\infty} \sum_{j=1}^{\infty} g\left(x_{i}, y_{j}\right) p_{i j}$
> 2）如果$(X,Y)$为二维离散型随机变量，联合密度函数为$f(x,y)$，
> 则$E[g(X, Y)]=\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} g(x, y) f(x, y) \mathrm{d} x \mathrm{~d} y$



特别的，我们取$g(X,Y) = X$和$g(X,Y)=Y$。即已知$(X,Y)$分布或者密度函数时可求$E(X)$和$E(Y)$.

可以按照方法二求解：

> 以二维连续型为例，设联合密度函数为$f(x,y)$，则：
> $E(X)=\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} x f(x, y) \mathrm{d} x \mathrm{~d} y$
> $E(Y)=\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} y f(x, y) \mathrm{d} x \mathrm{d} y$

当然也可以按照方法一求解：

> 先求X和Y各自的边缘密度$f_X(x)$和$f_Y(y)$，
> 再利用$E(X)=\int_{-\infty}^{+\infty} x f_{X}(x) \mathrm{d} x$和$E(Y)=\int_{-\infty}^{+\infty} y f_{Y}(y) \mathrm{d} y$求解

#### 高维随机变量$(X_1,X_2, \cdots, X_n)$的函数$g(X_1,X_2, \cdots, X_n)$的期望

类似二维的情况，有相同的两种方法。并且有类似的定理存在。

### 期望的性质

#### $E(C)=C$

设C为常数，则$E(C)=C$

#### $E(kX)= k E(X)$

设k为常数，X为随机变量，则$E(kX)= k E(X)$

#### $E(X+Y)=E(X)+E(Y)$

对于随机变量X和Y，有$E(X+Y)=E(X)+E(Y)$

#### $E\left(X_{1}+X_{2}+\cdots+X_{n}\right)=E\left(X_{1}\right)+E\left(X_{2}\right)+\cdots+E\left(X_{n}\right)$

这是上一条的推论：对于n个随机变量$X_{1}+X_{2}+\cdots+X_{n}$，有$E\left(X_{1}+X_{2}+\cdots+X_{n}\right)=E\left(X_{1}\right)+E\left(X_{2}\right)+\cdots+E\left(X_{n}\right)$

#### 随机变量X与Y独立$\Rightarrow$ $E(X Y)=E(X) E(Y)$

设随机变量X和Y独立，则$E(X Y)=E(X) E(Y)$

> 证明：
> 二维连续型$(X,Y)$情况（离散型类似可证）
> 设$(X,Y)$的联合密度函数为$f(x,y)$,
> 边缘密度函数微分别$f_X(x)$和$f_Y(y)$,
> 由于X和Y独立，则$f(x,y)=f_X(x)f_Y(y)$，
> 则：
> $\begin{aligned} E(X Y) &=\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} x y f(x, y) \mathrm{d} x \mathrm{~d} y \\ &=\int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} x y f_{X}(x) f_{Y}(y) \mathrm{d} x \mathrm{~d} y \\ &=\left(\int_{-\infty}^{+\infty} x f_{X}(x) \mathrm{d} x\right)\left(\int_{-\infty}^{+\infty} y f_{Y}(y) \mathrm{d} y\right) \\ &=E(X) E(Y) \end{aligned}$

#### $X_1,X_2, \cdots, X_n$相互独立$\Rightarrow$ $E\left(X_{1} X_{2} \cdots X_{n}\right)=E\left(X_{1}\right) E\left(X_{2}\right) \cdots E\left(X_{n}\right)$

这是上一条的推论：设n个随机变量$X_1,X_2, \cdots, X_n$相互独立，则$E\left(X_{1} X_{2} \cdots X_{n}\right)=E\left(X_{1}\right) E\left(X_{2}\right) \cdots E\left(X_{n}\right)$

## 方差

## 协方差与相关系数

## 矩

## 协方差矩阵

## n维正态分布