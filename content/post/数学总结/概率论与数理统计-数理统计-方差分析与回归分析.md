---
title: "概率论与数理统计-数理统计-方差分析与回归分析"
date: 2020-11-25T05:24:48+08:00
draft: false
categories: ["数学"]
series:
- TODO
- 概率论与数理统计
tags: ["概率论与数理统计","数理统计", "方差分析", "回归分析"] 
markup: pandoc
math: true
---

# 概率论与数理统计-数理统计-方差分析与回归分析

//TODO

参考：[https://baike.baidu.com/item/%E5%9B%9E%E5%BD%92%E5%88%86%E6%9E%90](https://baike.baidu.com/item/回归分析)
参考：概率论与数理统计.浙大第四版

方差分析和回归分析都是数理统计中具有广泛应用的内容. 本章对它们的 最基本部分作一介绍.

（本章节最好直接阅读教材，这里总结并不到位。这里引入了许多统计量的分布，证明参见：概率论与数理统计.浙大第四版）

## 单因素试验的方差分析

在科学试验和生产实践中，影响一事物的因素往往是很多的。
例如，在化工生产中，有原料成分、原料剂量、催化剂、反应温度、压力溶液浓度、反应时间、机器设备及操作人员的水平等因素每一因素的改变都有可能影响产品的数量和质量。
有些因素影响较大，有些较小。为了使生产过程得以稳定，保证优质、高产就**有必要找出对产品质量有显著影响的那些因素**.为此，我们需进行试验。
**方差分析就是根据试验的结果进行分析，鉴别各个有关因素对试验结果影响的有效方法**.

### 单因素试验

在试验中，我们将要考察的指标称为**试验指标**（对应随机变量）。影响试验指标的条件称为**因素**.（随机变量的参数，作为自变量看待）
因素可分为两类，
一类是人们可以控制的（可控因素）；
一类是人们不能控制的（不可控因素）。例如，反应温度、原料剂量、溶液浓度等是可以控制的，而测量误差、气象条件等一般是难以控制的。以下我们所说的因素都是指可控因素。
因素所处的状态，称为该**因素的水平**（见下述各例）。
如果在一项试验的过程中只有一个因素在改变称为**单因素试验**，如果多于一个因素在改变称为**多因素试验**。

> 例子
> 设有三台机器,用来生产规格相同的铝合金薄板. 取样,测量薄板的 厚度精确至千分之一厘米.得结果：
> ![image-20201225000929380](https://picgo12138.oss-cn-hangzhou.aliyuncs.com/md/image-20201225000929380.png)
>
> 这里，试验的指标是薄板的厚度。机器为因素，不同的三台机器就是这个因素的三个不同的水平。
> 我们假定除机器这一因素外，材料的规格、操作人员的水平等其他条件都相同。这是单因素试验。
> 试验的目的是为了考察各台机器所生产的薄板的厚度有无显著的差异，即考察机器这一因素对厚度有无显著的影响。
> 如果厚度有显著差异，就表明机器这一因素对厚度的影响是显著的

本节仅限于讨论单因素试验.

### 方差分析法

> 在实际中试验的指标往往要受到一种或多种因素的影响。方差分析就是通过对试验数据进行分析，检验方差相同的多个（多于两个）正态总体的均值是否相等，用以判断各因素对试验指标的影响是否显著。方差分析按影响试验指标的因素的个数分为单因素方差分析、双因素方差分析和多因素方差分析，本章只介绍前面两种。

在上面例子中,我们在因素的每一个水平下进行独立试验,其结果是一个样本.
表中数据可**看成**来自**三个不同总体**(每个水平对应一个总体)的样本值. 将各个总体的均值依次记为 $\mu_{1}, \mu_{2}, \mu_{3}$. 
上面的例子的问题是：机器这一因素对厚度的影响是显著？按题意**需检验假设**：
$H_{0}: \mu_{1}=\mu_{2}=\mu_{3}$
$H_{1}: \mu_{1}, \mu_{2}, \mu_{3}$ 不全相等.

现在进而假设各总体均为正态变量,且各总体的方差相等,但参数均未知.
那么 这是一个检验同方差的多个正态总体均值是否相等的问题. 

下面所要讨论的方差分析法,就是解决这类问题（检验同方差的多个正态总体均值是否相等）的一种统计方法.

#### 单因素试验方差分析的数学模型与假设检验问题

现在开始讨论单因素试验的方差分析.
设因素 $A$ 有 $s$ 个水平 $A_{1}, A_{2}, \cdots, A_{s},$ 在水平 $A_{j}(j=1,2, \cdots, s)$ 下,进行 $n_{j}\left(n_{j} \geqslant 2\right)$ 次独立试验,得到如下图表的结果：
![image-20201225002130712](https://picgo12138.oss-cn-hangzhou.aliyuncs.com/md/image-20201225002130712.png)

我们**假定** : 各个水平 $A_{j}(j=1,2, \cdots, s)$ 下的样本 $X_{1 j}, X_{2 j}, \cdots, X_{n_{j} j}$ 来自具有相同方差 $\sigma^{2},$ 均值分别为 $\mu_{j}(j=1,2, \cdots, s)$ 的正态总体 $N\left(\mu_{j}, \sigma^{2}\right), \mu_{j}$ 与 $\sigma^{2}$ 未知. 且设不同水平 $A_{j}$ 下的样本之间相互独立.

由于 $X_{i j} \sim N\left(\mu_{j}, \sigma^{2}\right),$ 即有 $X_{i j}-\mu_{j} \sim N\left(0, \sigma^{2}\right),$ 故 $X_{i j}-\mu_{j}$ 可看成是随机误差. 
记 $X_{i j}-\mu_{j}=\varepsilon_{i j},$ 则 $X_{i j}$ 可写成：
$\left.\begin{array}{l}X_{i j}=\mu_{j}+\varepsilon_{i j} \\ \varepsilon_{i j} \sim N\left(0, \sigma^{2}\right), \text { 各 } \varepsilon_{i j} \text { 独立 }, \\ i=1,2, \cdots, n_{j}, j=1,2, \cdots, s,\end{array}\right\}$，
其中 $\mu_{j}$ 与 $\sigma^{2}$ 均为未知参数. 
上式称为**单因素试验方差分析的数学模型**. 这是本节的**研究对象**.

**方差分析的任务**是对于以上模型：
1）检验 $s$ 个总体 $N\left(\mu_{1}, \sigma^{2}\right), \cdots, N\left(\mu_{s}, \sigma^{2}\right)$ 的均值是否相等,即检验假设：
$H_{0}: \mu_{1}=\mu_{2}=\cdots=\mu_{s}$
$H_{1}: \mu_{1}, \mu_{2}, \cdots, \mu_{s}$ 不全相等.
2）作出未知参数 $\mu_{1}, \mu_{2}, \cdots, \mu_{s}, \sigma^{2}$ 的估计.

一些记号：
为了更好地讨论以上方差分析,我们将 $\mu_{1}, \mu_{2}, \cdots, \mu_{s}$ 的加权平均值$\frac{1}{n} \sum_{j=1}^{s} n_{j} \mu_{j}$ 记为 $\mu,$ 
即$\mu=\frac{1}{n} \sum_{j=1}^{s} n_{j} \mu_{j}$，其中 $n=\sum_{j=1}^{s} n_{j}, \mu$ 称为**总平均**. 
再引入$\delta_{j}=\mu_{j}-\mu, \quad j=1,2, \cdots, s$，此时有 $n_{1} \delta_{1}+n_{2} \delta_{2}+\cdots+n_{s} \delta_{s}=0, \delta_{j}$ 表示**水平 $A_{j}$ 下的总体平均值与总平均的差异**,习惯上将 $\delta_{j}$ 称为**水平 $A_{j}$ 的效应**.

有了上面的记号，**单因素试验方差分析的数学模型**可以改写成如下形式：
$\left.\begin{array}{l}X_{i j}=\mu+\delta_{j}+\varepsilon_{i j} \\ \varepsilon_{i j} \sim N\left(0, \sigma^{2}\right), \text { 各 } \varepsilon_{i j} \text { 独立 } \\ i=1,2, \cdots, n_{j}, j=1,2, \cdots, s \\ \sum_{j=1}^{s} n_{j} \delta_{j}=0\end{array}\right\}$

有了上面的记号，假设检验问题也可以描述成如下形式：
$H_{0}: \delta_{1}=\delta_{2}=\cdots=\delta_{s}=0$
$H_{1}: \delta_{1}, \delta_{2}, \cdots, \delta_s\text {, 不全为零 } .$

#### 平方和的分解

下面我们从平方和的分解着手,导出假设上面检验问题的检验统计量.

引入**总偏差平方和**：$S_{T}=\sum_{j=1}^{s} \sum_{i=1}^{n_{j}}\left(X_{i j}-\bar{X}\right)^{2}$，
其中$\bar{X}=\frac{1}{n} \sum_{j=1}^{s} \sum_{i=1}^{n_{j}} X_{i j}$是数据的总平均. 
$\mathrm{S}_{\mathrm{T}}$ 能**反映全部试验数据之间的差异**,因此 $S_{T}$ 又称为总变差.

记**水平 $A_{j}$ 下的样本平均值**为 $\bar{X}_{\cdot j},$ 即$\bar{X}_{\cdot j}=\frac{1}{n_{j}} \sum_{i=1}^{n_{j}} X_{i j}$

则可以改写**总偏差平方和**$S_{T}$：
$\begin{aligned} S_{T} &=\sum_{j=1}^{s} \sum_{i=1}^{n_{j}}\left[\left(X_{i j}-\bar{X}_{\cdot j}\right)+\left(\bar{X}_{\cdot j}-\bar{X}\right)\right]^{2} \\ &=\sum_{j=1}^{s} \sum_{i=1}^{n_{j}}\left(X_{i j}-\bar{X}_{\cdot j}\right)^{2}+\sum_{j=1}^{s} \sum_{i=1}^{n_{j}}\left(\bar{X}_{\cdot j}-\bar{X}\right)^{2}+2 \sum_{j=1}^{s} \sum_{i=1}^{n_{j}}\left(X_{i j}-\bar{X}_{\cdot j}\right)\left(\bar{X}_{\cdot j}-\bar{X}\right) \end{aligned}$
注意到上式第三项(即交叉项):
$2 \sum_{j=1}^{s} \sum_{i=1}^{n_{j}}\left(X_{i j}-\bar{X}_{\cdot j}\right)\left(\bar{X}_{\cdot j}-\bar{X}\right)$
$\quad=2 \sum_{i j}^{s}\left(\bar{X}_{\cdot j}-\bar{X}\right)\left[\sum_{i j}^{n_{j}}\left(X_{i j}-\bar{X}_{\cdot j}\right)\right]=2 \sum\left(\bar{X}_{\cdot j}-\bar{X}\right)\left(\sum_{i j}-n_{j} \bar{X}_{\cdot j}\right)=0$

平方和分解式：我们将**总偏差平方和**$S_{T}$分解成为：$S_{T}=S_{E}+S_{A}$，
其中$S_{E}=\sum_{j=1}^{s} \sum_{i=1}^{n_{j}}\left(X_{i j}-\bar{X}_{\cdot j}\right)^{2}$，$S_{E}$ 的各项 $\left(X_{i j}-\bar{X}_{\cdot j}\right)^{2}$ 表示**在水平 $A_{j}$ 下,样本观察值与样本均值的差异** 这是由随机误差所引起的. $S_{E}$ 叫做**误差平方和**.$,$
$S_{A}=\sum_{j=1}^{s} \sum_{i=1}^{n_{j}}\left(\bar{X}_{\cdot j}-\bar{X}\right)^{2}=\sum_{j=1}^{s} n_{j}\left(\bar{X}_{\cdot j}-\bar{X}\right)^{2}=\sum_{j=1}^{s} n_{j} \bar{X}_{\cdot j}^{2}-n \bar{X}^{2}$，$S_{A}$ 的各项 $n_{j}\left(\bar{X}_{\cdot j}-\bar{X}\right)^{2}$ 表示 **$A_{j}$ 水平下的样本平均值与数据总平均的差异**,这是由水平 $A_{j}$ 的效应的差异以及随机误差引起的. $S_{A}$ 叫做因素 $A$ 的**效应平方和**. 

#### $S_{E}, S_{A}$ 的统计特性

为了引出上面检验问题的检验统计量,我们依次来讨论 $S_{E}, S_{A}$ 的一些统计特性. 
$S_{E}=\sum_{i=1}^{n_{1}}\left(X_{i 1}-\bar{X}_{\cdot 1}\right)^{2}+\cdots+\sum_{i=1}^{n_{s}}\left(X_{i s}-\bar{X}_{\cdot s}\right)^{2}$

注意每个j列都是一个正态总体总体 $N\left(\mu_{j}, \sigma^{2}\right)$，
于是有$\frac{\sum_{i=1}^{n_{j}}\left(X_{i j}-\bar{X}_{\cdot j}\right)^{2}}{\sigma^{2}} \sim \chi^{2}\left(n_{j}-1\right)$

各列（各总体）互相独立，故$S_{E}$式中各平方和相互独立. 
由 $\chi^{2}$ 分布的可加性知$\frac{S_{E}}{\sigma^{2}} \sim \chi^{2}\left(\sum_{i=1}^{s}\left(n_{j}-1\right)\right)$,
即：$\frac{S_{E}}{\sigma^{2}} \sim \chi^{2}(n-s)$，

由上式还可知$S_{E}$ 的自由度为 $n-s,$ 且有$E\left(S_{E}\right)=(n-s) \sigma^{2}$



下面讨论 $\mathrm{S}_{A}$ 的统计特性,
$S_{A}=\sum_{j=1}^{s} \sum_{i=1}^{n_{j}}\left(\bar{X}_{\cdot j}-\bar{X}\right)^{2}=\sum_{j=1}^{s} n_{j}\left(\bar{X}_{\cdot j}-\bar{X}\right)^{2}=\sum_{j=1}^{s} n_{j} \bar{X}_{\cdot j}^{2}-n \bar{X}^{2}$，
我们看到 $\mathrm{S}_{A}$ 是 $s$ 个变量 $\sqrt{n_{j}}\left(\bar{X}_{\cdot j}-\bar{X}\right)(j=1$, $2, \cdots, s)$ 的平方和，
它们之间仅有一个线性约束条件：$\sum_{j=1}^{s} \sqrt{n_{j}}\left[\sqrt{n_{j}}\left(\bar{X}_{\cdot j}-\bar{X}\right)\right]=\sum_{j=1}^{s} n_{j}\left(\bar{X}_{\cdot j}-\bar{X}\right)=\sum_{j=1}^{s} \sum_{i=1}^{n_{j}} X_{i j}-n \bar{X}=0$，
故知 $S_{A}$ 的自由度是 $s-1 .$

由$\mu=\frac{1}{n} \sum_{j=1}^{s} n_{j} \mu_{j}$，$\bar{X}=\frac{1}{n} \sum_{j=1}^{s} \sum_{i=1}^{n_{j}} X_{i j}$，以及$X_{ij}$的独立性，
知$\bar{X} \sim N\left(\mu, \frac{\sigma^{2}}{n}\right)$

可求得：
$\begin{aligned} E\left(S_{A}\right) &=E\left[\sum_{j=1}^{s} n_{j} \bar{X}_{\cdot j}^{2}-n \bar{X}^{2}\right]=\sum_{j=1}^{s} n_{j} E\left(\bar{X}_{\cdot j}^{2}\right)-n E\left(\bar{X}^{2}\right) \\ &=\sum_{j=1}^{s} n_{j}\left[\frac{\sigma^{2}}{n_{j}}+\left(\mu+\delta_{j}\right)^{2}\right]-n\left(\frac{\sigma^{2}}{n}+\mu^{2}\right) \\ &=(s-1) \sigma^{2}+2 \mu \sum_{j=1}^{s} n_{j} \delta_{j}+n \mu^{2}+\sum_{j=1}^{s} n_{j} \delta_{j}^{2}-n \mu^{2} \end{aligned}$
由改写后的单因素试验方差分析的数学模型可知$\sum_{j=1}^{s} n_{j} \delta_{j}=0$，
即得$E\left(S_{A}\right)=(s-1) \sigma^{2}+\sum_{j=1}^{s} n_{j} \delta_{j}^{2}$。

进一步还可以证明 $S_{A}$ 与 $S_{E}$ 独立,且当 $H_{0}$ 为真时，$\frac{S_{A}}{\sigma^{2}} \sim \chi^{2}(s-1)$

#### 假设检验问题的拒绝域

现在我们可以来确定原假设检验问题的拒绝域了.

由$E\left(S_{A}\right)=(s-1) \sigma^{2}+\sum_{j=1}^{s} n_{j} \delta_{j}^{2}$知,
当 $H_{0}$ 为真时，$E\left(\frac{S_{A}}{s-1}\right)=\sigma^{2}$。
即 $\frac{S_{A}}{s-1}$ 是 $\sigma^{2}$ 的无偏估计. 而当 $H_{1}$ 为真时 $, \sum_{j=1}^{s} n_{j} \delta_{j}^{2}>0,$ 此时$E\left(\frac{S_{A}}{s-1}\right)=\sigma^{2}+\frac{1}{s-1} \sum_{j=1}^{s} n_{j} \delta_{j}^{2}>\sigma^{2}$

由$E\left(S_{E}\right)=(n-s) \sigma^{2}$知，
$E\left(\frac{S_{E}}{n-s}\right)=\sigma^{2}$，
即不管 $H_{0}$ 是否为真 $, \frac{S_{E}}{n-s}$ 都是 $\sigma^{2}$ 的无偏估计.

综上所述,分式 $F=\frac{S_{A} /(s-1)}{S_{E} /(n-s)}$ 的分子与分母独立,
分母 $\frac{S_{E}}{n-s}$ 不论 $H_{0}$ 是否为真,其数学期望总是 $\sigma^{2} .$ 
当 $H_{0}$ 为真时,分子的数学期望为 $\sigma^{2},$ 
当 $H_{0}$ 不真时， 由$E\left(\frac{S_{A}}{s-1}\right)=\sigma^{2}+\frac{1}{s-1} \sum_{j=1}^{s} n_{j} \delta_{j}^{2}>\sigma^{2}$知分子的取值有偏大的趋势. 
故知原检验问题的拒绝域具有形式$F=\frac{S_{A} /(s-1)}{S_{E} /(n-s)} \geqslant k$

其中 $k$ 由预先给定的显著性水平 $\alpha$ 确定.
$\frac{S_{E}}{\sigma^{2}} \sim \chi^{2}(n-s)$，$\frac{S_{A}}{\sigma^{2}} \sim \chi^{2}(s-1)$，以及$S_{E}$ 与 $S_{A}$ 的独立性知,
当 $H_{0}$ 为真时，$\frac{S_{A} /(s-1)}{S_{E} /(n-s)}=\frac{S_{A} / \sigma^{2}}{s-1} / \frac{S_{E} / \sigma^{2}}{n-s} \sim F(s-1, n-s)$
由此得原检验问题的拒绝域为$F=\frac{S_{A} /(s-1)}{S_{E} /(n-s)} \geqslant F_{\alpha}(s-1, n-s)$

上述分析的结果可排成下面图表的形式,称为**方差分析表**：
![image-20201225013652933](https://picgo12138.oss-cn-hangzhou.aliyuncs.com/md/image-20201225013652933.png)
图表中 $\bar{S}_{A}=S_{A} /(s-1), \bar{S}_{E}=S_{E} /(n-s)$ 分别称为 $S_{A}, S_{E}$ 的**均方**. 
另外,因在$S_{T}$ 中 $n$ 个变量 $X_{i j}-\bar{X}$ 之间仅满足一个约束条件$\bar{X}=\frac{1}{n} \sum_{j=1}^{s} \sum_{i=1}^{n_{j}} X_{i j}$,故 $S_{T}$ 的自由度为 $n-1 .$

在实际中,我们可以按以下较简便的公式来计算 $S_{T}, S_{A}$ 和 $S_{E}$.
记：
$T_{\cdot j}=\sum_{i=1}^{n_{j}} X_{i j}, j=1,2, \cdots, s$
$T_{\cdot \cdot}=\sum_{j=1}^{s} \sum_{i=1}^{n_{j}} X_{i j}$
即有：
$\left.\begin{array}{l}S_{T}=\sum_{j=1}^{s} \sum_{i=1}^{n_{j}} X_{i j}^{2}-n \bar{X}^{2}=\sum_{j=1}^{s} \sum_{i=1}^{n_{j}} X_{i j}^{2}-\frac{T^{2}}{n} \\ S_{A}=\sum_{j=1}^{s} n_{j} \bar{X}_{\cdot j}^{2}-n \bar{X}^{2}=\sum_{j=1}^{s} \frac{T_{\cdot j}^{2}}{n_{j}}-\frac{T_{\cdots}^{2}}{n} \\ S_{E}=S_{T}-S_{A}\end{array}\right\}$

#### 未知参数的估计

上面已讲到过,不管 $H_{0}$ 是否为真，$\hat{\sigma}^{2}=\frac{S_{E}}{n-s}$，是 $\sigma^{2}$ 的无偏估计.
由$\bar{X} \sim N\left(\mu, \frac{\sigma^{2}}{n}\right)$，$\bar{X}_{\cdot j}=\frac{1}{n_{j}} \sum_{i=1}^{n_{j}} X_{i j}$，知：
$E(\bar{X})=\mu$，
$E\left(\bar{X}_{\cdot j}\right)=\frac{1}{n_{j}} \sum_{i=1}^{n_{j}} E\left(X_{i j}\right)=\mu_{j}, j=1,2, \cdots, s$
故 $\hat{\mu}=\bar{X}, \quad \hat{\mu}_{j}=\bar{X}_{\cdot j}$ 分别是 $\mu, \mu_{j}$ 的无偏估计.

又若拒绝 $H_{0},$ 这意味着效应 $\delta_{1}, \delta_{2}, \cdots, \delta_{s}$ 不全为零. 
由于$\delta_{j}=\mu_{j}-\mu, \quad j=1,2, \cdots, s$，
知 $\hat{\delta}_{j}=\bar{X}_{\cdot j}-\bar{X}$ 是 $\delta_{j}$ 的无偏估计.
此时还有关系式$\sum_{j=1}^{s} n_{j} \hat{\delta}_{j}=\sum_{j=1}^{s} n_{j} \bar{X}_{\cdot j}-n \bar{X}=0$
当拒绝 $H_{0}$ 时,常需要作出两总体 $N\left(\mu_{j}, \sigma^{2}\right)$ 和 $N\left(\mu_{k}, \sigma^{2}\right), j \neq k$ 的均值差 $\mu_j-\mu_{k}=\delta_{j}-\delta_{k}$ 的区间估计。做法如下：
由于$E\left(\bar{X}_{\cdot j}-\bar{X}_{\cdot k}\right)=\mu_{j}-\mu_{k}$，$D\left(\bar{X}_{\cdot j}-\bar{X}_{\cdot k}\right)=\sigma^{2}\left(\frac{1}{n_{j}}+\frac{1}{n_{k}}\right)$，
且$\bar{X}_{\cdot j}-\bar{X}_{\cdot k} \frac{\vdash_{\sigma} \hat{\sigma}^{2}}{=} S_{E} /(n-s)$ 独立.
于是$\frac{\left(\bar{X}_{\cdot j}-\bar{X}_{\cdot k}\right)-\left(\mu_{j}-\mu_{k}\right)}{\bar{S}_{E}\left(\frac{1}{n_{j}}+\frac{1}{n_{k}}\right)}$
$\quad=\frac{\left(\bar{X}_{\cdot j}-\bar{X}_{\cdot k}\right)-\left(\mu_{j}-\mu_{k}\right)}{\sigma \sqrt{1 / n_{j}+1 / n_{k}}} / \sqrt{\frac{S_{E}}{\sigma^{2}} /(n-s)} \sim t(n-s)$
据此得均值差 $\mu_{j}-\mu_{k}=\delta_{j}-\delta_{k}$ 的置信水平为 $1-\alpha$ 的置信区间为：
$\left(\bar{X}_{\cdot j}-\bar{X}_{\cdot k} \pm t_{a / 2}(n-s) \sqrt{\bar{S}_{E}\left(\frac{1}{n_{j}}+\frac{1}{n_{k}}\right)}\right)$

## 双因素试验的方差分析

//TODO

## 一元线性回归

在客观世界中普遍存在着变量之间的关系变量之间的关系。一般来说可分为确定性的与非确定性的两种。
**确定性关系**是指变量之间的关系可以用函数关系来表达的。
另一种**非确定性的关系**即所谓**相关关系**。例如人的身高与体重之间存在着关系，一般来说，人高一些，体重要重一些，但同样高度的人，体重往往不相同。人的血压与年龄之间也存在着关系，但同年龄的人的血压往往不相同。气象中的温度与湿度之间的关系也是这样。这是因为我们涉及的变量（如体重、血压、湿度）是随机变量，上面所说的变量关系是非确定性的回归分析是研究相关关系的一种数学工具。它能帮助我们从一个变量取得的值去估计另一变量所取的值。

### 一元线性回归模型

设随机变量 $Y$ 与 $x$ 之间存在着某种相关关系. 这里, $x$ 是可以控制或可以精确观察的变量,如年龄、试验时的温度、施加的压力、电压与时间等.换句话说我 们可以随意指定 $n$ 个值 $x_{1}, x_{2}, \cdots, x_{n} .$ 因此我们干脆不把 $x$ 看成是随机变量,而 将它当作普通的变量. 本章中我们只讨论这种情况.

设**随机变量 Y（因变量）与普通变量 $x$ (自变量) 之间存在着相关关系**,由于 $Y$是随机变量,对于 $x$ 的各个确定值,Y 有它 的分布如下图：
![image-20201225102334470](https://picgo12138.oss-cn-hangzhou.aliyuncs.com/md/image-20201225102334470.png)
（图中 $C_{1}, C_{2}$ 分别是 $x_{1}$,$x_{2}$ 处 $Y$ 的概率密度曲线 $) .$ 
用 $F(y \mid x)$ 表示 当 $x$ 取确定的 $x$ 值时,所对应的 $Y$ 的分布函数，如果我们掌握了 $F(y \mid x)$ 随着 $x$ 的 取值而变化的规律,那么就能完全掌握 Y与 $x$ 之间的关系了. 然而这样做**往往比较复杂**.

作为一种近似,我们**转而去考察 $Y$ 的数学期望**,若 $Y$ 的数学期望 $E(Y)$ 存在,则其值随 $x$ 的取值而定，**它是 $x$ 的函 数**. 将这一函数记为 $\mu_{Y \mid x}$ 或 $\mu(x)$,称为 $Y$ 关于 $x$ 的**回归函数**（如上图中所示）. 
这样， 我们就**将讨论 $Y$ 与 $x$ 的相关关系的问题转换为讨论 $E(Y)=\mu(x)$ 与 $x$ 的函数关系**了.

我们知道,若 $\eta$ 是一个随机变量,则 $E\left[(\eta-c)^{2}\right]$ 作为 $c$ 的函数,在 $c=E(\eta)$ 时 $E\left[(\eta-c)^{2}\right]$ 达到踏小(参见第四章习题第 17 题). 这表明在一切 $x$ 的函数中以 回归函数 $\mu(x)$ 作为 $Y$ 的近似,其均方误差 $E\left[(Y-\mu(x))^{2}\right]$ 为最小. 因此,作为一 种近似,为了研究 $Y$ 与 $x$ 的关系转而去研究 $\mu(x)$ 与 $x$ 的关系是合适的.

在实际问题中,回归函数 $\mu(x)$ 一般是未知的,
**回归分析的任务**是在于根据 试验数据去估计回归函数,讨论有关的点估计、区间估计、假设检验等问题. 特别重要的是对随机变量 $Y$ 的观察值作出点预测和区间预测.

我们对于 $x$ 取定一组不完全相同的值 $x_{1}, x_{2}, \cdots, x_{n},$ 设 $Y_{1}, Y_{2}, \cdots, Y_{n}$ 分别是 在 $x_{1}, x_{2}, \cdots, x_{n}$ 处对 $Y$ 的独立观察结果,称$\left(x_{1}, Y_{1}\right),\left(x_{2}, Y_{2}\right), \cdots,\left(x_{n}, Y_{n}\right)$是一个样本 , 对应的样本值记为$\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{n}, y_{n}\right)$

> 注意：这里 $Y_{1}, Y_{2}, \cdots, Y_{n}$ 是相互独立的随机变量,但一般未必同分布，为方便计,也称 $\left(x_{1}, Y_{1}\right),\left(x_{2},\right.$ $\left.Y_{2}\right), \cdots,\left(x_{n}, Y_{n}\right)$ 是一个样本.

我们首先要解决的问题是如何利用样本来估计 $Y$ 关于 $x$ 的回归函数 $\mu(x) .$ 
为此,首先需要推测 $\mu(x)$ 的形式. 
在一些问题中,我们可以由专业知识知道 $\mu(x)$ 的形式. 
否则 $,$ 可将每对观察值 $\left(x_{i}, y_{i}\right)$ 在直角坐标系中描出它的相应的点(如下例子图所示)，这种图称为**散点图**. 散点图可以帮助我们粗略地看出 $\mu(x)$ 的 形式.

> 例子：
> 为研究某一化学反应过程中,温度 $x\left({ }^{\circ} \mathrm{C}\right)$ 对产品得率 $Y(\%)$ 的影响，测得数据如下：
> ![image-20201225112853528](https://picgo12138.oss-cn-hangzhou.aliyuncs.com/md/image-20201225112853528.png)
> 这里自变量 $x$ 是普通变量, $Y$ 是随机变 量.画出散点图如下图所示. 由图大致看出 $\mu(x)$ 具有线性函数 $a+b x$ 的形 式.
> ![image-20201225112557356](https://picgo12138.oss-cn-hangzhou.aliyuncs.com/md/image-20201225112557356.png)

设 $Y$ 关于 $x$ 的回归函数为 $\mu(x) .$ 利 用样本来估计 $\mu(x)$ 的问题称为求 $Y$ 关 于 $x$ 的**回归问题**. 特别,若 $\mu(x)$ 为线性 函数 $: \mu(x)=a+b x,$ 此时估计 $\mu(x)$ 的 问题称为求**一元线性回归问题**. 本节只讨论这个问题.

我们假设对于 $x$ (在某个区间内) 的每一个值有$Y \sim N\left(a+b x, \sigma^{2}\right)$，
其中 $a, b$ 及 $\sigma^{2}$ 都是不依赖于 $x$ 的未知参数. 
记 $\varepsilon=Y-(a+b x)$,对 $Y$ 作这样的正态假设,相当于假设：
$Y=a+b x+\varepsilon, \quad \varepsilon \sim N\left(0, \sigma^{2}\right)$，
其中未知参数 $a, b$ 及 $\sigma^{2}$ 都不依赖于 $x .$ (3. 2 ) 称为**一元线性回归模型**,其中 $b$ 称为**回归系数**.

上面的一元线性回归模型表明，因变量 $Y$ 由两部分组成,一部分是 $x$ 的线性函数 $a+b x,$ 另一部分 $\varepsilon \sim N\left(0, \sigma^{2}\right)$ 是随机误差,是人们不可控制的.

### $a, b$ 的估计

现用**最大似然估计法来估计未知参数 $a, b .$** 

取 $x$ 的 $n$ 个不全相同的值 $x_{1}, x_{2}, \cdots, x_{n}$ 作独立试验,得到样本 $\left(x_{1}, Y_{1}\right),\left(x_{2},\right.\left.Y_{2}\right), \cdots,\left(x_{n}, Y_{n}\right) .$ 
由一元线性回归模型得$Y_{i}=a+b x_{i}+\varepsilon_{i}, \varepsilon_{i} \sim N\left(0, \sigma^{2}\right),$ 各 $\varepsilon_{i}$ 相互独立.
也即 $Y_{i} \sim N\left(a+b x_{i}, \sigma^{2}\right), i=1,2, \cdots, n .$ 

由 $Y_{1}, Y_{2}, \cdots, Y_{n}$ 的独立性, 知 $Y_{1},$$Y_{2}, \cdots, Y_{n}$ 的联合密度为：
$\begin{aligned} L &=\prod_{i=1}^{n} \frac{1}{\sigma \sqrt{2 \pi}} \exp \left[-\frac{1}{2 \sigma^{2}}\left(y_{i}-a-b x_{i}\right)^{2}\right] \\ &=\left(\frac{1}{\sigma \sqrt{2 \pi}}\right)^{n} \exp \left[-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(y_{i}-a-b x_{i}\right)^{2}\right] \end{aligned}$

对于任意一组观察值 $y_{1}, y_{2}, \cdots, y_{n},$ 联合密度函数L就是样本的似然函数. 
显然,要 $L$ 取最大值,只要L式右端方括弧中的平方和部分为最小,即**只需函数$Q(a, b)=\sum_{i=1}^{n}\left(y_{i}-a-b x_{i}\right)^{2}$取最小值**。
取 $Q$ 分别关于 $a, b$ 的偏导数,并令它们等于零：
$\left.\begin{array}{l}\frac{\partial Q}{\partial a}=-2 \sum_{i=1}^{n}\left(y_{i}-a-b x_{i}\right)=0 \\ \frac{\partial Q}{\partial b}=-2 \sum_{i=1}^{n}\left(y_{i}-a-b x_{i}\right) x_{i}=0\end{array}\right\}$
得方程组（称为正规方程组）：
$\left\{\begin{array}{l}n a+\left(\sum_{i=1}^{n} x_{i}\right) b=\sum_{i=1}^{n} y_{i} \\ \left(\sum_{i=1}^{n} x_{i}\right) a+\left(\sum_{i=1}^{n} x_{i}^{2}\right) b=\sum_{i=1}^{n} x_{i} y_{i}\end{array}\right.$
由于 $x_{i}$ 不全相同,正规方程组的系数行列式：
$\left|\begin{array}{cc}n & \sum_{i=1}^{n} x_{i} \\ \sum_{i=1}^{n} x_{i} & \sum_{i=1}^{n} x_{i}^{2}\end{array}\right|=n \sum_{i=1}^{n} x_{i}^{2}-\left(\sum_{i=1}^{n} x_{i}\right)^{2}=n \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2} \neq 0$
于是正规方程组有唯一的一组解. 
解得 $b, a$ 的最大似然估计值为
$\left.\begin{array}{l}\hat{b}=\frac{n \sum_{i=1}^{n} x_{i} y_{i}-\left(\sum_{i=1}^{n} x_{i}\right)\left(\sum_{i=1}^{n} y_{i}\right)}{n \sum_{i=1}^{n} x_{i}^{2}-\left(\sum_{i=1}^{n} x_{i}\right)^{2}}=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} \\ \hat{a}=\frac{1}{n} \sum_{i=1}^{n} y_{i}-\frac{\hat{b}}{n} \sum_{i=1}^{n} x_{i}=\bar{y}-\hat{b} \bar{x}\end{array}\right\}$
其中 $\bar{x}=\frac{1}{n} \sum_{i=1}^{n} x_{i}, \quad \bar{y}=\frac{1}{n} \sum_{i=1}^{n} y_{i}$

为了理解和计算的方便，引入以下记号：
$\left.\begin{array}{l}S_{x x}=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}=\sum_{i=1}^{n} x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n} x_{i}\right)^{2} \\ S_{y y}=\sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}=\sum_{i=1}^{n} y_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n} y_{i}\right)^{2} \\ S_{x y}=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)=\sum_{i=1}^{n} x_{i} y_{i}-\frac{1}{n}\left(\sum_{i=1}^{n} x_{i}\right)\left(\sum_{i=1}^{n} y_{i}\right)\end{array}\right\}$
这样a，b的估计可以简记为：
$\left.\begin{array}{l}\hat{b}=\frac{S_{x y}}{S_{x x}} \\ \hat{a}=\bar{y}-\hat{b} \bar{x}\end{array}\right\}$

> 注意：如果 Y不且正态变量,可直接用$Q(a, b)=\sum_{i=1}^{n}\left(y_{i}-a-b x_{i}\right)^{2}$估计 $a, b,$ 使 $Y$ 的观察值 $y_{i}$ 与 $a+b x_{i}$ 偏差的平方和 $Q (a,b)$为最小.这种方法叫**最小二乘法**,它是求经验公式的一种常用方法.
> 若 $Y$ 是正态变量,则最小二乘法与最大似然估计法给出相同的结果.

在得到 $a, b$ 的估计 $\hat{a}, \hat{b}$ 后，对于给定的 $x,$ 我们就取 $\hat{a}+\hat{b} x$ 作为回归函数 $\mu(x)=a+b x$ 的估计, 
即 $\mu(x)=\hat{a}+\hat{b} x$,称为 $Y$ 关于 $x$ 的**经验回归函数**. 
记 $\hat{a}+\hat{b} x$ $=\hat{y},$ 方程$\hat{y}=\hat{a}+\hat{b} x$称为 $Y$ 关于 $x$ 的**经验回归方程**,简称**回归方程**,其图形称为**回归直线**.

将 $\hat{a}$ 的表达式代入$\hat{y}=\hat{a}+\hat{b} x$,则回归方程可写成$\hat{y}=\bar{y}+\hat{b}(x-\bar{x})$，
这表明,对于样本值 $\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{n}, y_{n}\right),$ 回归直线通过散点图的几何中心 $(\bar{x}, \bar{y})$.

### $\sigma^{2}$ 的估计

由$Y=a+b x+\varepsilon, \quad \varepsilon \sim N\left(0, \sigma^{2}\right)$可知：
$E\left\{[Y-(a+b x)]^{2}\right\}=E\left(\varepsilon^{2}\right)=D(\varepsilon)+[E(\varepsilon)]^{2}=\sigma^{2}$

这表示 $\sigma^{2}$ 愈小,以回归函数 $\mu(x)=a+b x$ 作为 $Y$ 的近似导致的均方误差就愈小. 
这样,利用回归函数 $\mu(x)=a+b x$ 去研究随机变量 $Y$ 与 $x$ 的关系就愈有效. 
然而 $\sigma^{2}$ 是未知的,因而我们需要利用样本去估计 $\sigma^{2} .$ 为了估计 $\sigma^{2},$ 先引入下述残差平方和.

记 $\hat{y}_{i}=\left.\hat{y}\right|_{x=x_{i}}=\hat{a}+\hat{b} x_{i},$ 称$y_{i}-\hat{y}_{i}$ 为 $x_{i}$ 处的**残差**. 
平方和$Q_{e}=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}=\sum_{i=1}^{n}\left(y_{i}-\hat{a}-\hat{b} x_{i}\right)^{2}$称为**残差平方和**. 它是经验回归函数在 $x_{i}$ 处的函数值 $\mu\left(x_{i}\right)=\hat{a}+\hat{b} x_{i}$ 与 $x_{i}$ 处的观察值 $y_{i}$ 的 偏差的平方和.
 ![image-20201225124345680](https://picgo12138.oss-cn-hangzhou.aliyuncs.com/md/image-20201225124345680.png)
为了计算 $Q_{e},$ 我们将 $Q_{e}$ 作如下的分解 :
$\begin{aligned} Q_{e}=& \sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}=\sum_{i=1}^{n}\left[y_{i}-\bar{y}-\hat{b}\left(x_{i}-\bar{x}\right)\right]^{2} \\=& \sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}-2 \hat{b} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right) \\ &+(\hat{b})^{2} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}=S_{y y}-2 \hat{b} S_{x y}+(\hat{b})^{2} S_{x x} \end{aligned}$

由b的估计$\left.\begin{array}{l}\hat{b}=\frac{S_{x y}}{S_{x x}} \\ \hat{a}=\bar{y}-\hat{b} \bar{x}\end{array}\right\}$，将$\hat{b}$代入$Q_e$得：

$Q_{e}=S_{y y}-\hat{b} S_{x y}$

在 $S_{x y}, S_{x y}$ 的表达式式中,将 $y_{i}$ 改为 $Y_{i}$$(i=1,2, \cdots, n),$ 并把它们分别记为 $S_{Y Y}, S_{x Y},$ 
即$S_{Y Y}=\sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}, S_{x Y}=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(Y_{i}-\bar{Y}\right)$

则残差平方和 $Q_{e}$ 的相应的统计量(仍记为 $\left.Q_{e}\right)$ 为$Q_{e}=S_{Y Y}-\hat{b} S_{x Y}$

残差平方和 $Q_{e}$ 服从分布$\frac{Q_{e}}{\sigma^{2}} \sim \chi^{2}(n-2)$，
于是$E\left(\frac{Q_{e}}{\sigma^{2}}\right)=n-2$
即知 $E\left(Q_{e} /(n-2)\right)=\sigma^{2} .$ 
这样就得到了 $\sigma^{2}$ 的无偏估计量 :$\widehat{\sigma}^{2}=\frac{Q_{e}}{n-2}=\frac{1}{n-2}\left(S_{Y Y}-\hat{b} S_{x Y}\right)$

### 线性假设的显著性检验

在以上的讨论中,我们假定 $Y$ 关于 $x$ 的回归 $\mu(x)$ 具有形式 $a+b x$,
在处理实际问题时 $, \mu(x)$ 是否为 $x$ 的线性函数,首先要根据有关专业知识和实践来判断， 其次就要根据实际观察得到的数据运用假设检验的方法来判断. 
这就是说,求得的线性回归方程是否具有实用价值,一般来说,需要经过假设检验才能确定. 
若线性假设$Y=a+b x+\varepsilon, \quad \varepsilon \sim N\left(0, \sigma^{2}\right)$符合实际,则 $b$ 不应为零,因为若 $b=0,$ 则 $E(Y)=\mu(x)$ 就不依赖于 $x$ 了. 
因此我们需要检验假设$\begin{array}{ll}H_{0}: & b=0 \\ H_{1}: & b \neq 0\end{array}$，
我们使用 $t$ 检验法来进行检验. 我们有$\hat{b} \sim N\left(b, \sigma^{2} / S_{x x}\right)$。
又由$\frac{Q_{e}}{\sigma^{2}} \sim \chi^{2}(n-2)$，$\hat{\sigma}^{2}=\frac{Q_{e}}{n-2}=\frac{1}{n-2}\left(S_{Y Y}-\hat{b} S_{x Y}\right)$，
知$\frac{(n-2) \hat{\sigma}^{2}}{\sigma^{2}}=\frac{Q_{e}}{\sigma^{2}} \sim \chi^{2}(n-2)$

且 $\hat{b}$ 与 $Q_{e}$ 独立，故有$\frac{\hat{b}-b}{\sqrt{\sigma^{2} / S_{x x}}} / \sqrt{\frac{(n-2) \hat{\sigma}^{2}}{\sigma^{2}} /(n-2)} \sim t(n-2)$
即$\frac{\hat{b}-b}{\hat{\sigma}} \sqrt{S_{x x}} \sim t(n-2)$

当 $H_{0}$ 为真时 $b=0,$ 此时$t=\frac{\hat{b}}{\hat{\sigma}} \sqrt{S_{x x}} \sim t(n-2)$。
且 $E(\hat{b})=b=0,$ 即得 $H_{0}$ 的**拒绝域**为$|t|=\frac{|\hat{b}|}{\hat{\sigma}} \sqrt{S_{x x}} \geqslant t_{a / 2}(n-2)$，此处 $\alpha$ 为显著性水平.

当假设 $H_{0}: b=0$ 被拒绝时,认为回归效果是显著的,反之,就认为回归效果 不显著. 
回归效果不显著的原因可能有如下几种：
$1^{\circ}$ 影响 $Y$ 取值的,除 $x$ 及随机误差外还有其他不可忽略的因素.
$2^{\circ} E(Y)$ 与 $x$ 的关系不是线性的,而存在着其他的关系. 
$3^{\circ} Y$ 与 $x$ 不存在关系.

因此需要进一步的分析原因,分别处理.

### 系数 $b$ 的置信区间

当回归效果显著时,我们常需要对系数 $b$ 作区间估计. 事实上,可由$\frac{\hat{b}-b}{\hat{\sigma}} \sqrt{S_{x x}} \sim t(n-2)$得到 $b$ 的置信水平为 $1-\alpha$ 的置信区间为$\left(\hat{b} \pm t_{a / 2}(n-2) \times \frac{\hat{\sigma}}{\sqrt{S_{x x}}}\right)$

### 回归函数 $\mu(x)=a+b x$ 函数值的点估计和置信区间

设 $x_{0}$ 是自变量 $x$ 的某一指定值. 
根据回归方程$\hat{y}=\hat{a}+\hat{b} x$，可以用经验回归函数 $\hat{y}=\widehat{\mu(x)}$ $=\hat{a}+\hat{b} x$ 在 $x_{0}$ 的函数值 $\left.\hat{y}_{0}=\mu \widehat{\left(x_{0}\right.}\right)=\hat{a}+\hat{b} x_{0}$ 作为 $\mu\left(x_{0}\right)=a+b x_{0}$ 的点估计.
即$\hat{y}_{0}=\mu\left(x_{0}\right)=\hat{a}+\hat{b} x_{0}$,
考虑相应的估计量$\hat{Y}_{0}=\hat{a}+\hat{b} x_{0}$，
由$\hat{Y}_{0}=\hat{a}+\hat{b} x_{0}=\bar{Y}+\hat{b}\left(x_{0}-\bar{x}\right) \sim N\left(a+b x_{0},\left[\frac{1}{n}+\frac{\left(x_{0}-\bar{x}\right)^{2}}{S_{x r}}\right] \sigma^{2}\right)$，
知$E\left(\hat{Y}_{0}\right)=a+b x_{0},$ 因此这一估计量是无偏的.

下面来求 $\mu\left(x_{0}\right)$$=a+b x_{0}$ 的置信区间.
由$\hat{Y}_{0}=\hat{a}+\hat{b} x_{0}=\bar{Y}+\hat{b}\left(x_{0}-\bar{x}\right) \sim N\left(a+b x_{0},\left[\frac{1}{n}+\frac{\left(x_{0}-\bar{x}\right)^{2}}{S_{x r}}\right] \sigma^{2}\right)$，
知$\frac{\hat{Y}_{0}-\left(a+b x_{0}\right)}{\sigma \sqrt{\frac{1}{n}+\frac{\left(x_{0}-\bar{x}\right)^{2}}{S_{x x}}}} \sim N(0,1)$

由$\frac{Q_{e}}{\sigma^{2}} \sim \chi^{2}(n-2)$，$\hat{\sigma}^{2}=\frac{Q_{e}}{n-2}=\frac{1}{n-2}\left(S_{Y Y}-\hat{b} S_{x Y}\right)$，
知$\frac{(n-2) \hat{\sigma}^{2}}{\sigma^{2}}=\frac{Q_{e}}{\sigma^{2}} \sim \chi^{2}(n-2)$

若 $Y_{0}=a+b x_{0}+\varepsilon_{0}$ 与 $Y_{1}, \cdots, Y_{n}$ 独立,则 $Y_{0}, \hat{Y}_{0}, Q_e$ 相互独立.
由上述结论知 $Q_{e}, \hat{Y}_{0}$ 相互独立. 于是$\frac{\hat{Y}_{0}-\left(a+b x_{0}\right)}{\sigma \sqrt{\frac{1}{n}+\frac{\left(x_{0}-\bar{x}\right)^{2}}{S_{x x}}}} / \sqrt{\frac{(n-2) \hat{\sigma}^{2}}{\sigma^{2}} /(n-2)} \sim t(n-2)$
即$\frac{\hat{Y}_{0}-\left(a+b x_{0}\right)}{\hat{\sigma} \sqrt{\frac{1}{n}+\frac{\left(x_{0}-\bar{x}\right)^{2}}{S_{x x}}}} \sim t(n-2)$

于是得到 $\mu\left(x_{0}\right)=a+b x_{0}$ 的置信水平为 $1-\alpha$ 的置信区间为$\left(\hat{Y}_{0} \pm t_{\alpha / 2}(n-2) \hat{\sigma} \sqrt{\frac{1}{n}+\frac{\left(x_{0}-\bar{x}\right)^{2}}{S_{x x}}}\right)$,
或即$\left(\hat{a}+\hat{b} x_{0} \pm t_{a / 2}(n-2) \hat{\sigma} \sqrt{\frac{1}{n}+\frac{\left(x_{0}-\bar{x}\right)^{2}}{S_{x x}}}\right)$
这一置信区间的长度是 $x_{0}$ 的函数,它随 $\left|x_{0}-\bar{x}\right|$ 的增加而增加 $,$ 当 $x_{0}=\bar{x}$ 时为 最短.

### $Y$ 的观察值的点预测和预测区间

若我们对指定点 $x=x_{0}$ 处因变量 $Y$ 的观察值 $Y_{0}$ 感兴趣,然而我们在 $x=x_{0}$ 处并未进行观察或者暂时无法观察. 经验回归函数的一个重要应用是,可利用它对因变量 $Y$ 的新观察值 $Y_{0}$ 进行点预测或区间预测.
若 $Y_{0}$ 是在 $x=x_{0}$ 处对 $Y$ 的观察结果,可知它满足 :$Y_{0}=a+b x_{0}+\varepsilon_{0}, \quad \varepsilon_{0} \sim N\left(0, \sigma^{2}\right)$
随机误差 $\varepsilon_{0}$ 可正也可负,其值无法预料,我们就用 $x_{0}$ 处的经验回归函数值$\hat{Y}_{0}=\mu\left(x_{0}\right)=\hat{a}+\hat{b} x_{0}$作为 $Y_{0}=a+b x_{0}+\varepsilon_{0}$ 的点预测. 

下面来求 $Y_{0}$ 的预测区间.

因 $Y_{0}$ 是将要做的一次独立试验的结果,因此它与已经得到的试验的结果$Y_{1}, Y_{2}, \cdots, Y_{n}$ 相互独立. 
由这样a，b的估计$\left.\begin{array}{l}\hat{b}=\frac{S_{x y}}{S_{x x}} \\ \hat{a}=\bar{y}-\hat{b} \bar{x}\end{array}\right\}$知 $\hat{b}$ 是 $Y_{1}, Y_{2}, \cdots, Y_{n}$ 的线性组合,
故 $\hat{Y}_{0}=$$\bar{Y}+\hat{b}\left(x_{0}-\bar{x}\right)$ 是 $Y_{1}, Y_{2}, \cdots, Y_{n}$ 的线性组合,故 $Y_{0}$ 与 $\hat{Y}_{0}$ 相互独立. 
由 $Y_{0}=a+b x_{0}+\varepsilon_{0}, \quad \varepsilon_{0} \sim N\left(0, \sigma^{2}\right)$和$\hat{Y}_{0}=\hat{a}+\hat{b} x_{0}=\bar{Y}+\hat{b}\left(x_{0}-\bar{x}\right) \sim N\left(a+b x_{0},\left[\frac{1}{n}+\frac{\left(x_{0}-\bar{x}\right)^{2}}{S_{s x}}\right] \sigma^{2}\right)$,
得$\hat{Y}_{0}-Y_{0} \sim N\left(0,\left[1+\frac{1}{n}+\frac{\left(x_{0}-\bar{x}\right)^{2}}{S_{x x}}\right] \sigma^{2}\right)$，
即$\frac{\hat{Y}_{0}-Y_{0}}{\sigma \sqrt{1+\frac{1}{n}+\frac{\left(x_{0}-\bar{x}\right)^{2}}{S_{x x}}}} \sim N(0,1)$

再由$\frac{(n-2) \hat{\sigma}^{2}}{\sigma^{2}}=\frac{Q_{e}}{\sigma^{2}} \sim \chi^{2}(n-2)$，及 $Y_{0}, \hat{Y}_{0}, Q_{.}$ 的相互独立性
知$\frac{\hat{Y}_{0}-Y_{0}}{\sigma \sqrt{1+\frac{1}{n}+\frac{\left(x_{0}-\bar{x}\right)^{2}}{S_{x x}}}} / \sqrt{\frac{(n-2) \hat{\sigma}^{2}}{\sigma^{2}} /(n-2)} \sim t(n-2)$
即$\frac{\hat{Y}_{0}-Y_{0}}{\hat{\sigma} \sqrt{1+\frac{1}{n}+\frac{\left(x_{0}-\bar{x}\right)^{2}}{S_{x x}}}} \sim t(n-2)$

于是对于给定的置信水平 $1-\alpha$ 有$P\left\{\frac{\left|\hat{Y}_{0}-Y_{0}\right|}{\hat{\sigma} \sqrt{1+\frac{1}{n}+\frac{\left(x_{0}-\bar{x}\right)^{2}}{S_{x x}}}} \leqslant t_{a / 2}(n-2)\right\}=1-\alpha$
或$P\left\{\hat{Y}_{0}-t_{a / 2}(n-2) \hat{\sigma} \sqrt{1+\frac{1}{n}+\frac{\left(x_{0}-\bar{x}\right)^{2}}{S_{x x}}}<Y_{0}\right.$$\left.<\hat{Y}_{0}+t_{a / 2}(n-2) \hat{\sigma} \sqrt{1+\frac{1}{n}+\frac{\left(x_{0}-\bar{x}\right)^{2}}{S_{x x}}}\right\}=1-\alpha$

则区间$\left(\hat{Y}_{0} \pm t_{a / 2}(n-2) \hat{\sigma} \sqrt{1+\frac{1}{n}+\frac{\left(x_{0}-\bar{x}\right)^{2}}{S_{x x}}}\right)$，
即$\left(\hat{a}+\hat{b} x_{0} \pm t_{a / 2}(n-2) \hat{\sigma} \sqrt{\left.1+\frac{1}{n}+\frac{\left(x_{0}-\bar{x}\right)^{2}}{S_{x x}}\right)}\right.$称为$Y_{0}$ 的置信水平为 $1-\alpha$ 的预测区间.

这一预测区间的长度是 $x_{0}$ 的函数,它随 $\left|x_{0}-\bar{x}\right|$ 的增加而增加. 当 $x_{0}=\bar{x}$ 时为最短. 将上面得区间与回归函数$\mu\left(x_{0}\right)=a+b x_{0}$ 的置信区间比较， 知道在相同的置信水平下,回归函数值 $\mu\left(x_{0}\right)$ 的置信区间要比 $Y_{0}$ 的预测区间要 短. 这是因为 $Y_{0}=a+b x_{0}+\varepsilon_{0}$ 比 $\mu\left(x_{0}\right)=a+b x_{0}$ 多了一项 $\varepsilon_{0}$ 的缘故.

### 可化为一元线性回归的例子

以上讨论了一元线性回归问题,在实际中常会遇到更为复杂的回归问题,但 在某些情况下,可以通过适当的变量变换,可以将它化成一元线性回归来处理. 下面介绍几种常见的可转化为一元线性回归的模型.

#### $Y=\alpha \mathrm{e}^{\beta x} \cdot \varepsilon, \quad \ln \varepsilon \sim N\left(0, \sigma^{2}\right)$

其中 $\alpha, \beta, \sigma^{2}$ 是与 $x$ 无关的未知参数.

将 $Y=\alpha \mathrm{e}^{-\beta x} \cdot \varepsilon$ 两边取对数,得$\ln Y=\ln \alpha+\beta x+\ln \varepsilon$
令 $\ln Y=Y^{\prime}, \ln \alpha=a, \beta=b, x=x^{\prime}, \ln \varepsilon=\varepsilon^{\prime},$ 
上面的对数式可转化为一元线性回归模型：$Y^{\prime}=a+b x^{\prime}+\varepsilon^{\prime}, \quad \varepsilon^{\prime} \sim N\left(0, \sigma^{2}\right)$

#### $Y=\alpha x^{\beta} \cdot \varepsilon, \quad \ln \varepsilon \sim N\left(0, \sigma^{2}\right)$

其中 $\alpha, \beta, \sigma^{2}$ 是与 $x$ 无关的未知参数. 

将 $Y=\alpha x^{\beta} \cdot \varepsilon$ 两边取对数,得$\ln Y=\ln \alpha+\beta \ln x+\ln \varepsilon$
令 $\ln Y=Y^{\prime}, \ln \alpha=a, \beta=b, \ln x=x^{\prime}, \ln \varepsilon=\varepsilon^{\prime},$ 
上面的对数式可转化为一元线性回归模型：$Y^{\prime}=a+b x^{\prime}+\varepsilon^{\prime}, \quad \varepsilon^{\prime} \sim N\left(0, \sigma^{2}\right)$

#### $Y=\alpha+\beta h(x)+\varepsilon, \quad \varepsilon \sim N\left(0, \sigma^{2}\right)$

其中 $\alpha, \beta, \sigma^{2}$ 是与 $x$ 无关的未知参数. $h(x)$ 是 $x$ 的已知函数 $,$ 
令 $\alpha=a, \beta=b, h(x)$ $=x^{\prime},$ 可转化为一元线性回归模型：$Y=a+b x^{\prime}+\varepsilon, \quad \varepsilon \sim N\left(0, \sigma^{2}\right)$



若在原模型下，例如在原模型 下，对于 $(x, Y)$ 有样本 $\left(x_{1}, y_{1}\right),\left(x_{2},\right.$ $\left.y_{2}\right), \cdots,\left(x_{n}, y_{n}\right)$ 就相当于在新模型下有样本 $\left(x_{1}^{\prime}, y_{1}\right),\left(x_{2}^{\prime}, y_{2}\right), \cdots,\left(x_{n}^{\prime},\right.$$\left.y_{n}\right),$ 其中 $x_{i}^{\prime}=h\left(x_{i}\right) .$ 
于是就能利用上节的方法来估计 $a, b$ 或对 $b$ 作假设检验,或对 $Y$ 进行预测. 在得到 $Y$ 关于 $x^{\prime}$ 的回归方程后,再将原自变量 $x$ 代回,就得到$Y$ 关于 $x$ 的回归方程,它的图形是一条曲线,也称为**曲线回归方程**.

之前所讨论的**一元线性回归模型**是$Y=a+b x+\varepsilon, \quad \varepsilon \sim N\left(0, \sigma^{2}\right)$。
一般情况,**一元回归模型**为$Y=\mu\left(x ; \theta_{1}, \theta_{2}, \cdots, \theta_{p}\right)+\varepsilon, \quad \varepsilon \sim N\left(0, \sigma^{2}\right)$，其中 $\theta_{1}, \theta_{2}, \cdots, \theta_{p}, \sigma^{2}$ 是与 $x$ 无关的未知参数.

如果回归函数 $\mu\left(x ; \theta_{1}, \theta_{2}, \cdots, \theta_{p}\right)$ 是参数 $\theta_{1}, \theta_{2}, \cdots, \theta_{p}$ 的线性函数 $($ 不必是 $x$的线性函数 $)$,则称该一元回归模型为**线性回归模型**;
若 $\mu\left(x ; \theta_{1}, \theta_{2}, \cdots, \theta_{p}\right)$ 是 $\theta_{1}, \theta_{2}, \cdots, \theta_{p}$的非线性函数,则称为**非线性回归模型**.

上面提到的模型$Y=\alpha+\beta h(x)+\varepsilon, \quad \varepsilon \sim N\left(0, \sigma^{2}\right)$是线性回归模型，
而模型$Y=\alpha \mathrm{e}^{\beta x} \cdot \varepsilon, \quad \ln \varepsilon \sim N\left(0, \sigma^{2}\right)$以及$Y=\alpha x^{\beta} \cdot \varepsilon, \quad \ln \varepsilon \sim N\left(0, \sigma^{2}\right)$都不是线性回归模型,但是它们都能经过变量变换转化为线 性回归模型. 
又如$Y=\theta_{1} \mathrm{e}^{\theta_{2} x}+\varepsilon, \quad \varepsilon \sim N\left(0, \sigma^{2}\right)$它是非线性回归模型. 它不能经过变量变换将它转化为线性回归模型,称为**本 质的非线性回归模型**,
又如$Y=\left(\theta_{1}+\theta_{2} x+\theta_{3} x^{2}\right)^{-1}+\varepsilon \quad($ Holliday 模型 $),$
又如$Y=\frac{\theta_{1}}{1+\exp \left(\theta_{2}-\theta_{3}^{x}\right)}+\varepsilon \quad($ Logistic 模型 $),$
都是本质的非线性回归模型. 非线性回归模型的解法参见《近代非线性回归分析》(韦博成,南京 : 东南大学出版社,1989).

## 多元线性回归

在实际问题中,随机变量 $Y$ 往往与多个普通变量 $x_{1}, x_{2}, \cdots, x_{p} \quad(p>1)$ 有关. 
对于自变量 $x_{1}, x_{2}, \cdots, x_{p}$ 的一组确定的值,Y有它的分布.
若 $Y$ 的数学期望存在,则它是 $x_{1}, x_{2}, \cdots, x_{p}$ 的函数,记为 $\mu_{Y \mid x_{1}}, x_{2}, \cdots, x_{p}$ 或 $\mu\left(x_{1}, x_{2}, \cdots, x_{p}\right),$ 它就是 $Y$
关于 $x$ 的回归函数. 
我们感兴趣的是 $\mu\left(x_{1}, x_{2}, \cdots, x_{p}\right)$ 是 $x_{1}, x_{2}, \cdots, x_{p}$ 的线性函 数的情况. 

在这里,仅讨论下述多元线性回归模型 :$Y=b_{0}+b_{1} x_{1}+\cdots+b_{p} x_{p}+\varepsilon, \varepsilon \sim N\left(0, \sigma^{2}\right)$
其中 $b_{0}, b_{1}, \cdots, b_{p}, \sigma^{2}$ 都是与 $x_{1}, x_{2}, \cdots, x_{p}$ 无关的未知参数.

设$\left(x_{11}, x_{12}, \cdots, x_{1 p}, y_{1}\right), \cdots,\left(x_{n 1}, x_{n 2}, \cdots, x_{n p}, y_{n}\right)$是一个样本.

 和一元线性回归的情况一样,我们用最大似然估计法来估计参数.
即取 $\hat{b}_{0}, \hat{b}_{1}, \cdots, \hat{b}_{p}$ 使当 $b_{0}=\hat{b}_{0}, b_{1}=\hat{b}_{1}, \cdots, b_{p}=\hat{b}_{p}$ 时$Q=\sum_{i=1}^{n}\left(y_{i}-b_{0}-b_{1} x_{i 1}-\cdots-b_{p} x_{i p}\right)^{2}$达到最小.

求 $Q$ 分别关于 $b_{0}, b_{1}, \cdots, b_{p}$ 的偏导数,并令它们等于实,得：
$\left.\begin{array}{r}\frac{\partial Q}{\partial b_{0}}=-2 \sum_{i=1}^{n}\left(y_{i}-b_{0}-b_{1} x_{i 1}-\cdots-b_{p} x_{i p}\right)=0, \\ \frac{\partial Q}{\partial b_{j}}=-2 \sum_{i=1}^{n}\left(y_{i}-b_{0}-b_{1} x_{i 1}-\cdots-b_{p} x_{i j}\right) x_{i j}=0, \\ j=1,2, \cdots, p .\end{array}\right\}$

化简得：
$\left.\begin{array}{l}b_{0} n+b_{1} \sum_{i=1}^{n} x_{i 1}+b_{2} \sum_{i=1}^{n} x_{i 2}+\cdots+b_{p} \sum_{i=1}^{n} x_{i p}=\sum_{i=1}^{n} y_{i} \\ b_{0} \sum_{i=1}^{n} x_{i 1}+b_{1} \sum_{i=1}^{n} x_{i 1}^{2}+b_{2} \sum_{i=1}^{n} x_{i 1} x_{i 2}+\cdots+b_{p} \sum_{i=1}^{n} x_{i 1} x_{i p}=\sum_{i=1}^{n} x_{i 1} y_{i} \\ \vdots \\ b_{0} \sum_{i=1}^{n} x_{i p}+b_{1} \sum_{i=1}^{n} x_{i_{i}} x_{i 1}+b_{2} \sum_{i=1}^{n} x_{i p} x_{i 2}+\cdots+b_{p} \sum_{i=1}^{n} x_{i_{p}}^{2}=\sum_{i=1}^{n} x_{i p} y_{i} .\end{array}\right\}$
称为**正规方程组**.
为了求解的方便,常要将上方程组写成矩阵的形式.

为此，引入矩阵：
$\boldsymbol{X}=\left(\begin{array}{ccccc}1 & x_{11} & x_{12} & \cdots & x_{1 p} \\ 1 & x_{2 i} & x_{22} & \cdots & x_{2 p} \\ \vdots & \vdots & \vdots & & \vdots \\ 1 & x_{n 1} & x_{n 2} & \cdots & x_{n p}\end{array}\right), \boldsymbol{Y}=\left[\begin{array}{c}y_{1} \\ y_{2} \\ \vdots \\ y_{n}\end{array}\right), \boldsymbol{B}=\left(\begin{array}{c}b_{0} \\ b_{1} \\ \vdots \\ b_{p}\end{array}\right) .$

而有：
$\boldsymbol{X}^{\mathrm{T}} \boldsymbol{X}=\left(\begin{array}{cccc}1 & 1 & \cdots & 1 \\ x_{11} & x_{21} & \cdots & x_{n 1} \\ \vdots & \vdots & & \vdots \\ x_{1 p} & x_{2 p} & \cdots & x_{n p}\end{array}\right)\left(\begin{array}{ccccc}1 & x_{11} & x_{12} & \cdots & x_{1 p} \\ 1 & x_{21} & x_{22} & \cdots & x_{2 p} \\ \vdots & \vdots & \vdots & & \vdots \\ 1 & x_{n 1} & x_{n 2} & \cdots & x_{n p}\end{array}\right)$
$=\left(\begin{array}{cccc}n & \sum_{i=1}^{n} x_{i 1} & \cdots & \sum_{i=1}^{n} x_{i p} \\ \sum_{i=1}^{n} x_{i 1} & \sum_{i=1}^{n} x_{i 1}^{2} & \cdots & \sum_{i=1}^{n} x_{i 1} x_{i p} \\ \vdots & \vdots & & \vdots \\ \sum_{i=1}^{n} x_{i p} & \sum_{i=1}^{n} x_{i p} x_{i 1} & \cdots & \sum_{i=1}^{n} x_{i p}^{2}\end{array}\right),$

$\boldsymbol{X}^{\mathrm{T}} \boldsymbol{Y}=\left(\begin{array}{cccc}1 & 1 & \cdots & 1 \\ x_{11} & x_{21} & \cdots & x_{n 1} \\ \vdots & \vdots & & \vdots \\ x_{1 p} & x_{2 p} & \cdots & x_{n p}\end{array}\right)\left(\begin{array}{c}y_{1} \\ y_{2} \\ \vdots \\ y_{n}\end{array}\right)=\left(\begin{array}{c}\sum_{i=1}^{n} y_{i} \\ \sum_{i=1}^{n} x_{i 1} y_{i} \\ \vdots \\ \sum_{i=1}^{n} x_{i p} y_{i}\end{array}\right) .$

于是正规方程组可以写成：
$\boldsymbol{X}^{\mathrm{T}} \boldsymbol{X B}=\boldsymbol{X}^{\mathrm{T}} \boldsymbol{Y}$

上式两边左乘 $\boldsymbol{X}^{\mathrm{T}} \boldsymbol{X}$ 的逆矩阵 $\left(\boldsymbol{X}^{\mathrm{T}} \boldsymbol{X}\right)^{-1}$ (设$\left(\boldsymbol{X}^{\mathrm{T}} \boldsymbol{X}\right)^{-1}$ 存在 $)$ 解得：
$\hat{\boldsymbol{B}}=\left(\begin{array}{c}\hat{b}_{0} \\ \hat{b}_{1} \\ \vdots \\ \hat{b}_{p}\end{array}\right)=\left(\boldsymbol{X}^{\mathrm{T}} \boldsymbol{X}\right)^{-1} \mathbf{X}^{\mathrm{T}} \boldsymbol{Y}$
这就是我们需要求的 $\left(b_{0}, b_{1}, \cdots, b_{p}\right)^{\mathrm{T}}$ 的最大似然估计. 

我们取$\hat{b}_{0}+\hat{b}_{1} x_{1}+\cdots+\hat{b}_{p} x_{p} \stackrel{\text { 记成 }}{=}\hat{y}$作为 $\mu\left(x_{1}, x_{2}, \cdots, x_{p}\right)=b_{0}+b_{1} x_{1}+\cdots+b_{p} x_{p}$ 的估计. 
方程$\hat{y}=\hat{b}_{0}+\hat{b}_{1} x_{1}+\cdots+\hat{b}_{p} x_{p}$称为 **$p$ 元经验线性回归方程**,简称**回归方程**.

> 例子
> 下面给出了某种产品每件平均单价 $Y$ (元) 与批量 $x$ (件) 之间的关系的一组数据：
> ![image-20201225165628628](https://picgo12138.oss-cn-hangzhou.aliyuncs.com/md/image-20201225165628628.png)
> 画出散点图：
> ![image-20201225165659292](https://picgo12138.oss-cn-hangzhou.aliyuncs.com/md/image-20201225165659292.png)
> 我们选取模型$Y=b_{0}+b_{1} x+b_{2} x^{2}+\varepsilon, \varepsilon \sim N\left(0, \sigma^{2}\right)$来拟合它。
>
> 现在来求它的回归方程：
> 令 $x_{1}=x, x_{2}=x^{2}$，则模型可以写成$Y=b_{0}+b_{1} x_{1}+b_{2} x_{2}+\varepsilon, \quad \varepsilon \sim N\left(0, \sigma^{2}\right)$，
> 这是一个二元线性回归模型。
> 现在有：
> $\boldsymbol{X}=\left(\begin{array}{lll}1 & 20 & 400 \\ 1 & 25 & 625 \\ 1 & 30 & 900 \\ 1 & 35 & 1 & 225 \\ 1 & 40 & 1 & 600 \\ 1 & 50 & 2 & 500 \\ 1 & 60 & 3 & 600 \\ 1 & 65 & 4 & 225 \\ 1 & 70 & 4 & 900 \\ 1 & 75 & 5 & 625 \\ 1 & 80 & 6 & 400 \\ 1 & 90 & 8 & 100\end{array}\right), \boldsymbol{Y}=\left(\begin{array}{l}1.81 \\ 1.70 \\ 1.65 \\ 1.55 \\ 1.48 \\ 1.40 \\ 1.30 \\ 1.26 \\ 1.24 \\ 1.21 \\ 1.20 \\ 1.18\end{array}\right), \boldsymbol{B}=\left[\begin{array}{l}b_{0} \\ b_{1} \\ b_{2}\end{array}\right) .$
> 经计算：
> $\boldsymbol{X}^{\mathrm{T}} \boldsymbol{X}=\left(\begin{array}{ccc}12 & 640 & 40100 \\ 640 & 40100 & 2779000 \\ 40100 & 2779000 & 204 702 500\end{array}\right)$
> ![image-20201225170109081](https://picgo12138.oss-cn-hangzhou.aliyuncs.com/md/image-20201225170109081.png)
> $\Delta=1.41918 \times 10^{11}$，
> 得正规方程组的解：
> $\hat{\boldsymbol{B}}=\left(\begin{array}{l}\hat{b}_{0} \\ \hat{b}_{1} \\ \hat{b}_{2}\end{array}\right)=\left(\boldsymbol{X}^{\mathrm{T}} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\mathrm{T}} \boldsymbol{Y}=\left(\boldsymbol{X}^{\mathrm{T}} \boldsymbol{X}\right)^{-1} \quad\left(\begin{array}{l}16.98 \\ 851.3 \\ 51162\end{array}\right)$
> $=\left(\begin{array}{lll}2.198  266  29 \\ -0.022  522  36 \\ 0.000  125  07\end{array}\right)$
> 于是得到回归方程为$\hat{y}=2.19826629-0.02252236 x+0.00012507 x^{2} .$

像一元线性回归一样,多元线性回归模型$Y=b_{0}+b_{1} x_{1}+\cdots+b_{p} x_{p}+\varepsilon, \varepsilon \sim N\left(0, \sigma^{2}\right)$往往是一种假定,为了考察这一假定是否符合实际观察结果,还需进行以下的假设检验 :
$H_{0}: \quad b_{1}=b_{2}=\cdots=b_{p}=0$
$\mathrm{H}_{1}: b_{i}$ 不全为零
若在显著性水平 $\alpha$ 下拒绝 $H_{0} .$ 我们就认为回归效果是显著的.

另外,也与一元线性回归一样,多元线性回归方程的一个重要应用是确定给定点 $\left(x_{01}, x_{02}, \cdots, x_{0 p}\right)$ 处对应的 $Y$ 的观察值的预测区间.

最后我们指出，在实际问题中，与Y有关的因素往往很多，如果将它们都取作自变量必然会导致所得到的回归方程很庞大。
实际上，有些自变量对Y的影响很小，如果将这些自变量剔除，不但能使回归方程较为简洁，便于应用，且能明确哪些因素（即自变量）的改变对Y有显著的影响，从而使人们对事物有进一步的认识。
通常可用**逐步回归法**达到这一目的。
上述关于模型的线性假设的检验、观察值的预测区间、逐步回归等内容，读者可参阅华东师范大学出版社出版的《回归分析及其试验设计》一书

在实际中，需要考虑的影响Y的因素较多，即自变量的个数较多。因此要求解一个多元线性回归的问题，计算工作量是相当大的，这就需要借助于计算机来进行计算。